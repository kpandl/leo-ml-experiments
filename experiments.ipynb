{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_and_extract_dataset(url, save_path, folder_path):\n",
    "    \"\"\"Download and extract dataset if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        decompressed_file_name = os.path.splitext(os.path.basename(save_path))[0]\n",
    "        decompressed_file_path = os.path.join(folder_path, decompressed_file_name)\n",
    "\n",
    "        with gzip.open(save_path, \"rb\") as f_in:\n",
    "            with open(decompressed_file_path, \"wb\") as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        print(f\"{decompressed_file_name} downloaded and extracted.\")\n",
    "    else:\n",
    "        print(f\"{os.path.basename(save_path)} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz already exists.\n",
      "train-labels-idx1-ubyte.gz already exists.\n",
      "t10k-images-idx3-ubyte.gz already exists.\n",
      "t10k-labels-idx1-ubyte.gz already exists.\n"
     ]
    }
   ],
   "source": [
    "# URLs and filenames\n",
    "file_info = [\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "        \"train-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "        \"train-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "        \"t10k-images-idx3-ubyte.gz\",\n",
    "    ),\n",
    "    (\n",
    "        \"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
    "        \"t10k-labels-idx1-ubyte.gz\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "folder_name = \"tmp/mnist\"\n",
    "folder_path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Download and extract each file\n",
    "for url, file_name in file_info:\n",
    "    path_to_save = os.path.join(folder_path, file_name)\n",
    "    download_and_extract_dataset(url, path_to_save, folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_idx3_ubyte_image_file(filename):\n",
    "    \"\"\"Read IDX3-ubyte formatted image data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_images = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_rows = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_cols = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2051:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        images = np.zeros((num_images, num_rows, num_cols), dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_images):\n",
    "            for r in range(num_rows):\n",
    "                for c in range(num_cols):\n",
    "                    pixel = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "                    images[i, r, c] = pixel\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_idx1_ubyte_label_file(filename):\n",
    "    \"\"\"Read IDX1-ubyte formatted label data.\"\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic_num = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "        num_labels = int.from_bytes(f.read(4), byteorder=\"big\")\n",
    "\n",
    "        if magic_num != 2049:\n",
    "            raise ValueError(f\"Invalid magic number: {magic_num}\")\n",
    "\n",
    "        labels = np.zeros(num_labels, dtype=np.uint8)\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            labels[i] = int.from_bytes(f.read(1), byteorder=\"big\")\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_images: (60000, 28, 28)\n",
      "Shape of train_labels: (60000,)\n",
      "Shape of test_images: (10000, 28, 28)\n",
      "Shape of test_labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.path.join(\n",
    "    os.getcwd(), folder_name\n",
    ")  # Adjust this path to where you stored the files\n",
    "\n",
    "train_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"train-images-idx3-ubyte\")\n",
    ")\n",
    "train_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"train-labels-idx1-ubyte\")\n",
    ")\n",
    "test_images = read_idx3_ubyte_image_file(\n",
    "    os.path.join(folder_path, \"t10k-images-idx3-ubyte\")\n",
    ")\n",
    "test_labels = read_idx1_ubyte_label_file(\n",
    "    os.path.join(folder_path, \"t10k-labels-idx1-ubyte\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shape of train_images: {train_images.shape}\"\n",
    ")  # Should output \"Shape of train_images: (60000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of train_labels: {train_labels.shape}\"\n",
    ")  # Should output \"Shape of train_labels: (60000,)\"\n",
    "print(\n",
    "    f\"Shape of test_images: {test_images.shape}\"\n",
    ")  # Should output \"Shape of test_images: (10000, 28, 28)\"\n",
    "print(\n",
    "    f\"Shape of test_labels: {test_labels.shape}\"\n",
    ")  # Should output \"Shape of test_labels: (10000,)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the datasets to PyTorch tensors, and get a validation set\n",
    "(We use PyTorch instead of sci-kit learn to train sparse neural networks with L1 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to pytorch tensors\n",
    "import torch\n",
    "\n",
    "train_images_tensor_initial = torch.from_numpy(train_images).float()\n",
    "train_labels_tensor_initial = torch.from_numpy(train_labels).long()\n",
    "test_images_tensor = torch.from_numpy(test_images).float()\n",
    "test_labels_tensor = torch.from_numpy(test_labels).long()\n",
    "\n",
    "# seed the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# shuffle the training dataset\n",
    "indices = torch.randperm(train_images_tensor_initial.shape[0])\n",
    "train_images_tensor_shuffled = train_images_tensor_initial[indices]\n",
    "train_labels_tensor_shuffled = train_labels_tensor_initial[indices]\n",
    "\n",
    "# get a 10% validation set\n",
    "validation_size = int(train_images_tensor_shuffled.shape[0] * 0.1)\n",
    "validation_images_tensor = train_images_tensor_shuffled[:validation_size]\n",
    "validation_labels_tensor = train_labels_tensor_shuffled[:validation_size]\n",
    "train_images_tensor = train_images_tensor_shuffled[validation_size:]\n",
    "train_labels_tensor = train_labels_tensor_shuffled[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels tensor shape: torch.Size([54000])\n",
      "Validation labels tensor shape: torch.Size([6000])\n",
      "Test labels tensor shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train labels tensor shape:\", train_labels_tensor.shape)\n",
    "print(\"Validation labels tensor shape:\", validation_labels_tensor.shape)\n",
    "print(\"Test labels tensor shape:\", test_labels_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract feature representations of the dataset\n",
    "\n",
    "(We transform the bounding box images to 12x12 images, defined by the new_size variable. There is a trade-off in circuit constraints and ML model accuracy. You can increase the image size which will lead to a higher accuracy at the cost of more circuit constraints and thus longer proving times.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_bounding_box(img):\n",
    "    \"\"\"\n",
    "    Extract the bounding box from an MNIST image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): 2D numpy array representing the MNIST image.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray): Cropped image with the bounding box.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert torch image to numpy array\n",
    "    img = img.numpy()\n",
    "\n",
    "    # Find the rows and columns where the image has non-zero pixels\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "\n",
    "    # Find the first and last row and column indices where the image has non-zero pixels\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "\n",
    "    # Return the cropped image\n",
    "    return img[rmin : rmax + 1, cmin : cmax + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_resized_datasets(\n",
    "    train_images_tensor, validation_images_tensor, test_images_tensor, new_size\n",
    "):\n",
    "    num_train = len(train_images_tensor)\n",
    "    num_test = len(test_images_tensor)\n",
    "    num_val = len(validation_images_tensor)\n",
    "\n",
    "    train_images_tensor_resized = np.zeros((num_train, new_size**2))\n",
    "    validation_images_tensor_resized = np.zeros((num_val, new_size**2))\n",
    "    test_images_tensor_resized = np.zeros((num_test, new_size**2))\n",
    "\n",
    "    for i in range(num_train):\n",
    "        cropped_image = get_bounding_box(train_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = cv2.resize(\n",
    "            cropped_image_uint8, (new_size, new_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        train_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    for i in range(num_val):\n",
    "        cropped_image = get_bounding_box(validation_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = cv2.resize(\n",
    "            cropped_image_uint8, (new_size, new_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        validation_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    for i in range(num_test):\n",
    "        cropped_image = get_bounding_box(test_images_tensor[i].reshape(28, 28))\n",
    "        cropped_image_uint8 = np.clip(cropped_image, 0, 255).astype(np.uint8)\n",
    "        resized_image = cv2.resize(\n",
    "            cropped_image_uint8, (new_size, new_size), interpolation=cv2.INTER_AREA\n",
    "        )\n",
    "        test_images_tensor_resized[i, :] = resized_image.flatten()\n",
    "\n",
    "    return (\n",
    "        train_images_tensor_resized,\n",
    "        validation_images_tensor_resized,\n",
    "        test_images_tensor_resized,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = 12\n",
    "train_images_resized, val_images_resized, test_images_resized = get_resized_datasets(\n",
    "    train_images_tensor, validation_images_tensor, test_images_tensor, new_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_haar_features(image):\n",
    "    # raise value error if the image is not square\n",
    "    if image.shape[0] != image.shape[1]:\n",
    "        raise ValueError(\"The input image must be square.\")\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # Sliding window\n",
    "    for i in range(0, image.shape[0], 3):  # Slide vertically with a step of 3\n",
    "        for j in range(0, image.shape[0], 3):  # Slide horizontally with a step of 3\n",
    "\n",
    "            if i + 6 > image.shape[0] or j + 6 > image.shape[0]:\n",
    "                continue\n",
    "\n",
    "            # Extract 6x6 window\n",
    "            window = image[i : i + 6, j : j + 6]\n",
    "\n",
    "            # Horizontal feature\n",
    "            horizontal_feature_value = np.sum(window[0:3, :]) - np.sum(window[3:6, :])\n",
    "\n",
    "            # Vertical feature\n",
    "            vertical_feature_value = np.sum(window[:, 0:3]) - np.sum(window[:, 3:6])\n",
    "\n",
    "            features.append(horizontal_feature_value)\n",
    "            features.append(vertical_feature_value)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def aspect_ratio(image, threshold=0.5):\n",
    "    # Threshold the image to create a binary representation\n",
    "    bin_image = image > threshold\n",
    "    # Find the bounding box\n",
    "    row_indices, col_indices = np.nonzero(bin_image)\n",
    "    max_row, min_row = np.max(row_indices), np.min(row_indices)\n",
    "    max_col, min_col = np.max(col_indices), np.min(col_indices)\n",
    "\n",
    "    # Calculate the aspect ratio of the bounding box\n",
    "    width = max_col - min_col + 1\n",
    "    height = max_row - min_row + 1\n",
    "\n",
    "    if height == 0:  # To avoid division by zero\n",
    "        return 1.0\n",
    "\n",
    "    return width / height\n",
    "\n",
    "\n",
    "from scipy.ndimage import label\n",
    "\n",
    "\n",
    "def num_regions_below_threshold(image, threshold=0.5):\n",
    "    # Threshold the image so that pixels below the threshold are set to 1\n",
    "    # and those above the threshold are set to 0.\n",
    "    bin_image = image < threshold\n",
    "\n",
    "    # Use connected components labeling\n",
    "    labeled_array, num_features = label(bin_image)\n",
    "\n",
    "    # Return the number of unique regions\n",
    "    # (subtracting 1 as one of the labels will be the background)\n",
    "    return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "num_train = len(train_images_tensor)\n",
    "num_val = len(validation_images_tensor)\n",
    "num_test = len(test_images_tensor)\n",
    "\n",
    "aspect_ratio_train = np.zeros(num_train)\n",
    "aspect_ratio_val = np.zeros(num_val)\n",
    "aspect_ratio_test = np.zeros(num_test)\n",
    "\n",
    "num_white_regions_train = np.zeros(num_train)\n",
    "num_white_regions_val = np.zeros(num_val)\n",
    "num_white_regions_test = np.zeros(num_test)\n",
    "\n",
    "haar_1 = compute_haar_features(train_images_resized[0].reshape(new_size, new_size))\n",
    "len_haar_features = len(haar_1)\n",
    "\n",
    "haar_train = np.zeros((num_train, len_haar_features))\n",
    "haar_val = np.zeros((num_val, len_haar_features))\n",
    "haar_test = np.zeros((num_test, len_haar_features))\n",
    "\n",
    "for i in range(num_train):\n",
    "    aspect_ratio_train[i] = aspect_ratio(train_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_train[i] = num_regions_below_threshold(\n",
    "        train_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_train[i, :] = compute_haar_features(\n",
    "        train_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    aspect_ratio_val[i] = aspect_ratio(\n",
    "        validation_images_tensor[i].reshape(28, 28).numpy()\n",
    "    )\n",
    "    num_white_regions_val[i] = num_regions_below_threshold(\n",
    "        validation_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_val[i, :] = compute_haar_features(\n",
    "        val_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    aspect_ratio_test[i] = aspect_ratio(test_images_tensor[i].reshape(28, 28).numpy())\n",
    "    num_white_regions_test[i] = num_regions_below_threshold(\n",
    "        test_images_tensor[i].reshape(28, 28)\n",
    "    )\n",
    "    haar_test[i, :] = compute_haar_features(\n",
    "        test_images_resized[i, :].reshape(new_size, new_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at the images, and the computed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape torch.Size([28, 28])\n",
      "Label tensor(0)\n",
      "Resized image\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape (12, 12)\n",
      "Haar features: [-1413. -1567.   789.   -89.    88.   590.   461.   929.   743.  -101.\n",
      "  -311. -1153. -1046.  1922. -1208.  -974.  1161.  1035.]\n",
      "Shape of Haar features: (18,)\n",
      "Aspect ratio: 0.7\n",
      "Number of white regions: 2.0\n"
     ]
    }
   ],
   "source": [
    "image_id = 0\n",
    "\n",
    "image = train_images_tensor[image_id].reshape(28, 28)\n",
    "\n",
    "print(\"Original image\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image.shape)\n",
    "print(\"Label\", train_labels_tensor[image_id])\n",
    "\n",
    "print(\"Resized image\")\n",
    "\n",
    "image_resized = train_images_resized[image_id].reshape(new_size, new_size)\n",
    "\n",
    "plt.imshow(image_resized, cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Image shape\", image_resized.shape)\n",
    "\n",
    "print(\"Haar features:\", haar_train[image_id, :])\n",
    "print(\"Shape of Haar features:\", haar_train[image_id, :].shape)\n",
    "\n",
    "print(\"Aspect ratio:\", aspect_ratio_train[image_id])\n",
    "print(\"Number of white regions:\", num_white_regions_train[image_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's merge all features into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute datasets\n",
    "\n",
    "train_features = np.zeros((num_train, len_haar_features + 2))\n",
    "val_features = np.zeros((num_val, len_haar_features + 2))\n",
    "test_features = np.zeros((num_test, len_haar_features + 2))\n",
    "\n",
    "for i in range(num_train):\n",
    "    train_features[i, :] = np.hstack(\n",
    "        (haar_train[i, :], aspect_ratio_train[i], num_white_regions_train[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_val):\n",
    "    val_features[i, :] = np.hstack(\n",
    "        (haar_val[i, :], aspect_ratio_val[i], num_white_regions_val[i])\n",
    "    )\n",
    "\n",
    "for i in range(num_test):\n",
    "    test_features[i, :] = np.hstack(\n",
    "        (haar_test[i, :], aspect_ratio_test[i], num_white_regions_test[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features: (54000, 20)\n",
      "First training feature vector: [-1.413e+03 -1.567e+03  7.890e+02 -8.900e+01  8.800e+01  5.900e+02\n",
      "  4.610e+02  9.290e+02  7.430e+02 -1.010e+02 -3.110e+02 -1.153e+03\n",
      " -1.046e+03  1.922e+03 -1.208e+03 -9.740e+02  1.161e+03  1.035e+03\n",
      "  7.000e-01  2.000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of training features:\", train_features.shape)\n",
    "print(\"First training feature vector:\", train_features[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_normalized = torch.tensor(scaler.fit_transform(train_features)).float()\n",
    "val_features_normalized = torch.tensor(scaler.transform(val_features)).float()\n",
    "test_features_normalized = torch.tensor(scaler.transform(test_features)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training feature vector (normalized): tensor([-1.2143, -0.8321,  0.4906,  0.1937, -0.2191,  0.0082,  0.4311,  1.1292,\n",
      "         0.6659,  0.1091, -0.3643, -1.2731, -0.6324,  2.4261, -0.8677, -0.8842,\n",
      "         1.0577,  0.2177, -0.4527,  0.4035])\n"
     ]
    }
   ],
   "source": [
    "print(\"First training feature vector (normalized):\", train_features_normalized[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the neural network and the training and test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(test_features_normalized)\n",
    "        _, predicted = torch.max(test_outputs.data, 1)\n",
    "        accuracy = accuracy_score(test_labels, predicted.numpy())\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# Define the PyTorch neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop with L1 regularization\n",
    "    lambda_l1 = 0.0001  # L1 regularization coefficient\n",
    "\n",
    "    validation_losses = []\n",
    "    epoch = 0\n",
    "\n",
    "    model_states = []\n",
    "\n",
    "    while True:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_features_normalized)\n",
    "\n",
    "        loss = criterion(outputs, train_labels_tensor)\n",
    "\n",
    "        # Add L1 regularization\n",
    "        l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "        for param in model.parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        loss += lambda_l1 * l1_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}], Loss: {loss.item():.4f}, validation loss: {validation_losses[-1]:.4f}\"\n",
    "            )\n",
    "\n",
    "        # store model state\n",
    "        model_states.append(model.state_dict())\n",
    "\n",
    "        # Compute validation loss\n",
    "        with torch.no_grad():\n",
    "            outputs = model(val_features_normalized)\n",
    "            loss = criterion(outputs, validation_labels_tensor)\n",
    "            validation_losses.append(loss.item())\n",
    "\n",
    "        # Check for early stopping if no improvement in validation loss in last 10 epochs\n",
    "        if epoch > 10 and validation_losses[-1] > validation_losses[-10]:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    best_model_state = model_states[np.argmin(validation_losses)]\n",
    "    model.load_state_dict(best_model_state)\n",
    "    validation_loss_of_best_model = validation_losses[np.argmin(validation_losses)]\n",
    "\n",
    "    return model, epoch, validation_loss_of_best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create and train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100], Loss: 1.6674, validation loss: 1.6664\n",
      "Epoch [200], Loss: 0.9882, validation loss: 0.9826\n",
      "Epoch [300], Loss: 0.6444, validation loss: 0.6358\n",
      "Epoch [400], Loss: 0.4973, validation loss: 0.4875\n",
      "Epoch [500], Loss: 0.4247, validation loss: 0.4148\n",
      "Epoch [600], Loss: 0.3842, validation loss: 0.3740\n",
      "Epoch [700], Loss: 0.3591, validation loss: 0.3488\n",
      "Epoch [800], Loss: 0.3424, validation loss: 0.3318\n",
      "Epoch [900], Loss: 0.3305, validation loss: 0.3197\n",
      "Epoch [1000], Loss: 0.3217, validation loss: 0.3106\n",
      "Epoch [1100], Loss: 0.3146, validation loss: 0.3035\n",
      "Epoch [1200], Loss: 0.3087, validation loss: 0.2977\n",
      "Epoch [1300], Loss: 0.3038, validation loss: 0.2929\n",
      "Epoch [1400], Loss: 0.2994, validation loss: 0.2887\n",
      "Epoch [1500], Loss: 0.2955, validation loss: 0.2850\n",
      "Epoch [1600], Loss: 0.2918, validation loss: 0.2816\n",
      "Epoch [1700], Loss: 0.2883, validation loss: 0.2783\n",
      "Epoch [1800], Loss: 0.2848, validation loss: 0.2751\n",
      "Epoch [1900], Loss: 0.2814, validation loss: 0.2717\n",
      "Epoch [2000], Loss: 0.2779, validation loss: 0.2682\n",
      "Epoch [2100], Loss: 0.2745, validation loss: 0.2647\n",
      "Epoch [2200], Loss: 0.2712, validation loss: 0.2611\n",
      "Epoch [2300], Loss: 0.2678, validation loss: 0.2574\n",
      "Epoch [2400], Loss: 0.2645, validation loss: 0.2537\n",
      "Epoch [2500], Loss: 0.2613, validation loss: 0.2503\n",
      "Epoch [2600], Loss: 0.2583, validation loss: 0.2470\n",
      "Epoch [2700], Loss: 0.2553, validation loss: 0.2438\n",
      "Epoch [2800], Loss: 0.2523, validation loss: 0.2406\n",
      "Epoch [2900], Loss: 0.2496, validation loss: 0.2376\n",
      "Epoch [3000], Loss: 0.2472, validation loss: 0.2350\n",
      "Epoch [3100], Loss: 0.2450, validation loss: 0.2328\n",
      "Epoch [3200], Loss: 0.2429, validation loss: 0.2307\n",
      "Epoch [3300], Loss: 0.2411, validation loss: 0.2288\n",
      "Epoch [3400], Loss: 0.2395, validation loss: 0.2271\n",
      "Epoch [3500], Loss: 0.2381, validation loss: 0.2257\n",
      "Epoch [3600], Loss: 0.2368, validation loss: 0.2243\n",
      "Epoch [3700], Loss: 0.2355, validation loss: 0.2228\n",
      "Epoch [3800], Loss: 0.2343, validation loss: 0.2215\n",
      "Epoch [3900], Loss: 0.2332, validation loss: 0.2202\n",
      "Epoch [4000], Loss: 0.2321, validation loss: 0.2189\n",
      "Epoch [4100], Loss: 0.2312, validation loss: 0.2178\n",
      "Epoch [4200], Loss: 0.2302, validation loss: 0.2166\n",
      "Epoch [4300], Loss: 0.2293, validation loss: 0.2156\n",
      "Epoch [4400], Loss: 0.2284, validation loss: 0.2145\n",
      "Epoch [4500], Loss: 0.2276, validation loss: 0.2134\n",
      "Epoch [4600], Loss: 0.2268, validation loss: 0.2125\n",
      "Epoch [4700], Loss: 0.2261, validation loss: 0.2117\n",
      "Epoch [4800], Loss: 0.2253, validation loss: 0.2107\n",
      "Epoch [4900], Loss: 0.2247, validation loss: 0.2101\n",
      "Epoch [5000], Loss: 0.2240, validation loss: 0.2094\n",
      "Epoch [5100], Loss: 0.2233, validation loss: 0.2089\n",
      "Epoch [5200], Loss: 0.2227, validation loss: 0.2085\n",
      "Epoch [5300], Loss: 0.2221, validation loss: 0.2080\n",
      "Epoch [5400], Loss: 0.2216, validation loss: 0.2076\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.6301, validation loss: 1.6219\n",
      "Epoch [200], Loss: 0.9931, validation loss: 0.9894\n",
      "Epoch [300], Loss: 0.6472, validation loss: 0.6420\n",
      "Epoch [400], Loss: 0.4971, validation loss: 0.4891\n",
      "Epoch [500], Loss: 0.4230, validation loss: 0.4132\n",
      "Epoch [600], Loss: 0.3796, validation loss: 0.3684\n",
      "Epoch [700], Loss: 0.3522, validation loss: 0.3401\n",
      "Epoch [800], Loss: 0.3338, validation loss: 0.3210\n",
      "Epoch [900], Loss: 0.3209, validation loss: 0.3077\n",
      "Epoch [1000], Loss: 0.3112, validation loss: 0.2978\n",
      "Epoch [1100], Loss: 0.3036, validation loss: 0.2901\n",
      "Epoch [1200], Loss: 0.2974, validation loss: 0.2838\n",
      "Epoch [1300], Loss: 0.2920, validation loss: 0.2784\n",
      "Epoch [1400], Loss: 0.2872, validation loss: 0.2739\n",
      "Epoch [1500], Loss: 0.2828, validation loss: 0.2696\n",
      "Epoch [1600], Loss: 0.2788, validation loss: 0.2656\n",
      "Epoch [1700], Loss: 0.2751, validation loss: 0.2617\n",
      "Epoch [1800], Loss: 0.2717, validation loss: 0.2581\n",
      "Epoch [1900], Loss: 0.2685, validation loss: 0.2546\n",
      "Epoch [2000], Loss: 0.2657, validation loss: 0.2514\n",
      "Epoch [2100], Loss: 0.2631, validation loss: 0.2487\n",
      "Epoch [2200], Loss: 0.2607, validation loss: 0.2461\n",
      "Epoch [2300], Loss: 0.2585, validation loss: 0.2436\n",
      "Epoch [2400], Loss: 0.2564, validation loss: 0.2412\n",
      "Epoch [2500], Loss: 0.2544, validation loss: 0.2390\n",
      "Epoch [2600], Loss: 0.2524, validation loss: 0.2369\n",
      "Epoch [2700], Loss: 0.2505, validation loss: 0.2348\n",
      "Epoch [2800], Loss: 0.2487, validation loss: 0.2328\n",
      "Epoch [2900], Loss: 0.2469, validation loss: 0.2308\n",
      "Epoch [3000], Loss: 0.2452, validation loss: 0.2290\n",
      "Epoch [3100], Loss: 0.2437, validation loss: 0.2273\n",
      "Epoch [3200], Loss: 0.2423, validation loss: 0.2258\n",
      "Epoch [3300], Loss: 0.2409, validation loss: 0.2244\n",
      "Epoch [3400], Loss: 0.2395, validation loss: 0.2231\n",
      "Epoch [3500], Loss: 0.2382, validation loss: 0.2218\n",
      "Epoch [3600], Loss: 0.2371, validation loss: 0.2209\n",
      "Epoch [3700], Loss: 0.2359, validation loss: 0.2200\n",
      "Epoch [3800], Loss: 0.2348, validation loss: 0.2191\n",
      "Epoch [3900], Loss: 0.2336, validation loss: 0.2180\n",
      "Epoch [4000], Loss: 0.2325, validation loss: 0.2170\n",
      "Epoch [4100], Loss: 0.2314, validation loss: 0.2160\n",
      "Epoch [4200], Loss: 0.2304, validation loss: 0.2151\n",
      "Epoch [4300], Loss: 0.2294, validation loss: 0.2143\n",
      "Epoch [4400], Loss: 0.2284, validation loss: 0.2134\n",
      "Epoch [4500], Loss: 0.2275, validation loss: 0.2125\n",
      "Epoch [4600], Loss: 0.2266, validation loss: 0.2116\n",
      "Epoch [4700], Loss: 0.2257, validation loss: 0.2110\n",
      "Epoch [4800], Loss: 0.2248, validation loss: 0.2103\n",
      "Epoch [4900], Loss: 0.2240, validation loss: 0.2098\n",
      "Epoch [5000], Loss: 0.2233, validation loss: 0.2093\n",
      "Epoch [5100], Loss: 0.2226, validation loss: 0.2088\n",
      "Epoch [5200], Loss: 0.2220, validation loss: 0.2081\n",
      "Epoch [5300], Loss: 0.2214, validation loss: 0.2075\n",
      "Epoch [5400], Loss: 0.2209, validation loss: 0.2071\n",
      "Epoch [5500], Loss: 0.2203, validation loss: 0.2067\n",
      "Epoch [5600], Loss: 0.2198, validation loss: 0.2063\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.6430, validation loss: 1.6474\n",
      "Epoch [200], Loss: 0.9264, validation loss: 0.9290\n",
      "Epoch [300], Loss: 0.6092, validation loss: 0.6073\n",
      "Epoch [400], Loss: 0.4694, validation loss: 0.4635\n",
      "Epoch [500], Loss: 0.4043, validation loss: 0.3958\n",
      "Epoch [600], Loss: 0.3695, validation loss: 0.3602\n",
      "Epoch [700], Loss: 0.3485, validation loss: 0.3386\n",
      "Epoch [800], Loss: 0.3343, validation loss: 0.3239\n",
      "Epoch [900], Loss: 0.3238, validation loss: 0.3128\n",
      "Epoch [1000], Loss: 0.3154, validation loss: 0.3040\n",
      "Epoch [1100], Loss: 0.3082, validation loss: 0.2965\n",
      "Epoch [1200], Loss: 0.3020, validation loss: 0.2897\n",
      "Epoch [1300], Loss: 0.2963, validation loss: 0.2833\n",
      "Epoch [1400], Loss: 0.2911, validation loss: 0.2774\n",
      "Epoch [1500], Loss: 0.2862, validation loss: 0.2719\n",
      "Epoch [1600], Loss: 0.2813, validation loss: 0.2664\n",
      "Epoch [1700], Loss: 0.2766, validation loss: 0.2610\n",
      "Epoch [1800], Loss: 0.2722, validation loss: 0.2560\n",
      "Epoch [1900], Loss: 0.2679, validation loss: 0.2514\n",
      "Epoch [2000], Loss: 0.2640, validation loss: 0.2469\n",
      "Epoch [2100], Loss: 0.2603, validation loss: 0.2429\n",
      "Epoch [2200], Loss: 0.2570, validation loss: 0.2393\n",
      "Epoch [2300], Loss: 0.2539, validation loss: 0.2360\n",
      "Epoch [2400], Loss: 0.2510, validation loss: 0.2330\n",
      "Epoch [2500], Loss: 0.2482, validation loss: 0.2300\n",
      "Epoch [2600], Loss: 0.2456, validation loss: 0.2273\n",
      "Epoch [2700], Loss: 0.2433, validation loss: 0.2247\n",
      "Epoch [2800], Loss: 0.2412, validation loss: 0.2225\n",
      "Epoch [2900], Loss: 0.2392, validation loss: 0.2206\n",
      "Epoch [3000], Loss: 0.2375, validation loss: 0.2189\n",
      "Epoch [3100], Loss: 0.2359, validation loss: 0.2174\n",
      "Epoch [3200], Loss: 0.2344, validation loss: 0.2158\n",
      "Epoch [3300], Loss: 0.2329, validation loss: 0.2142\n",
      "Epoch [3400], Loss: 0.2315, validation loss: 0.2128\n",
      "Epoch [3500], Loss: 0.2301, validation loss: 0.2116\n",
      "Epoch [3600], Loss: 0.2289, validation loss: 0.2105\n",
      "Epoch [3700], Loss: 0.2276, validation loss: 0.2095\n",
      "Epoch [3800], Loss: 0.2264, validation loss: 0.2083\n",
      "Epoch [3900], Loss: 0.2252, validation loss: 0.2075\n",
      "Epoch [4000], Loss: 0.2241, validation loss: 0.2067\n",
      "Epoch [4100], Loss: 0.2231, validation loss: 0.2062\n",
      "Epoch [4200], Loss: 0.2222, validation loss: 0.2058\n",
      "Epoch [4300], Loss: 0.2214, validation loss: 0.2055\n",
      "Epoch [4400], Loss: 0.2206, validation loss: 0.2050\n",
      "Epoch [4500], Loss: 0.2198, validation loss: 0.2046\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.6168, validation loss: 1.6168\n",
      "Epoch [200], Loss: 0.9729, validation loss: 0.9684\n",
      "Epoch [300], Loss: 0.6309, validation loss: 0.6232\n",
      "Epoch [400], Loss: 0.4736, validation loss: 0.4638\n",
      "Epoch [500], Loss: 0.4011, validation loss: 0.3897\n",
      "Epoch [600], Loss: 0.3626, validation loss: 0.3506\n",
      "Epoch [700], Loss: 0.3390, validation loss: 0.3269\n",
      "Epoch [800], Loss: 0.3226, validation loss: 0.3106\n",
      "Epoch [900], Loss: 0.3103, validation loss: 0.2983\n",
      "Epoch [1000], Loss: 0.3007, validation loss: 0.2887\n",
      "Epoch [1100], Loss: 0.2927, validation loss: 0.2807\n",
      "Epoch [1200], Loss: 0.2861, validation loss: 0.2739\n",
      "Epoch [1300], Loss: 0.2804, validation loss: 0.2679\n",
      "Epoch [1400], Loss: 0.2753, validation loss: 0.2627\n",
      "Epoch [1500], Loss: 0.2708, validation loss: 0.2580\n",
      "Epoch [1600], Loss: 0.2668, validation loss: 0.2538\n",
      "Epoch [1700], Loss: 0.2631, validation loss: 0.2500\n",
      "Epoch [1800], Loss: 0.2597, validation loss: 0.2464\n",
      "Epoch [1900], Loss: 0.2564, validation loss: 0.2432\n",
      "Epoch [2000], Loss: 0.2534, validation loss: 0.2402\n",
      "Epoch [2100], Loss: 0.2504, validation loss: 0.2373\n",
      "Epoch [2200], Loss: 0.2477, validation loss: 0.2347\n",
      "Epoch [2300], Loss: 0.2452, validation loss: 0.2323\n",
      "Epoch [2400], Loss: 0.2428, validation loss: 0.2300\n",
      "Epoch [2500], Loss: 0.2406, validation loss: 0.2277\n",
      "Epoch [2600], Loss: 0.2385, validation loss: 0.2255\n",
      "Epoch [2700], Loss: 0.2366, validation loss: 0.2234\n",
      "Epoch [2800], Loss: 0.2348, validation loss: 0.2216\n",
      "Epoch [2900], Loss: 0.2330, validation loss: 0.2198\n",
      "Epoch [3000], Loss: 0.2313, validation loss: 0.2180\n",
      "Epoch [3100], Loss: 0.2298, validation loss: 0.2162\n",
      "Epoch [3200], Loss: 0.2283, validation loss: 0.2142\n",
      "Epoch [3300], Loss: 0.2268, validation loss: 0.2123\n",
      "Epoch [3400], Loss: 0.2252, validation loss: 0.2105\n",
      "Epoch [3500], Loss: 0.2237, validation loss: 0.2092\n",
      "Epoch [3600], Loss: 0.2222, validation loss: 0.2078\n",
      "Epoch [3700], Loss: 0.2208, validation loss: 0.2065\n",
      "Epoch [3800], Loss: 0.2194, validation loss: 0.2052\n",
      "Epoch [3900], Loss: 0.2182, validation loss: 0.2040\n",
      "Epoch [4000], Loss: 0.2171, validation loss: 0.2033\n",
      "Epoch [4100], Loss: 0.2160, validation loss: 0.2026\n",
      "Epoch [4200], Loss: 0.2152, validation loss: 0.2020\n",
      "Epoch [4300], Loss: 0.2144, validation loss: 0.2015\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.5262, validation loss: 1.5241\n",
      "Epoch [200], Loss: 0.8817, validation loss: 0.8724\n",
      "Epoch [300], Loss: 0.6006, validation loss: 0.5889\n",
      "Epoch [400], Loss: 0.4675, validation loss: 0.4552\n",
      "Epoch [500], Loss: 0.4006, validation loss: 0.3876\n",
      "Epoch [600], Loss: 0.3624, validation loss: 0.3488\n",
      "Epoch [700], Loss: 0.3379, validation loss: 0.3246\n",
      "Epoch [800], Loss: 0.3208, validation loss: 0.3080\n",
      "Epoch [900], Loss: 0.3082, validation loss: 0.2961\n",
      "Epoch [1000], Loss: 0.2983, validation loss: 0.2868\n",
      "Epoch [1100], Loss: 0.2902, validation loss: 0.2790\n",
      "Epoch [1200], Loss: 0.2834, validation loss: 0.2724\n",
      "Epoch [1300], Loss: 0.2774, validation loss: 0.2665\n",
      "Epoch [1400], Loss: 0.2722, validation loss: 0.2613\n",
      "Epoch [1500], Loss: 0.2675, validation loss: 0.2567\n",
      "Epoch [1600], Loss: 0.2631, validation loss: 0.2524\n",
      "Epoch [1700], Loss: 0.2590, validation loss: 0.2485\n",
      "Epoch [1800], Loss: 0.2553, validation loss: 0.2449\n",
      "Epoch [1900], Loss: 0.2519, validation loss: 0.2415\n",
      "Epoch [2000], Loss: 0.2488, validation loss: 0.2384\n",
      "Epoch [2100], Loss: 0.2461, validation loss: 0.2355\n",
      "Epoch [2200], Loss: 0.2436, validation loss: 0.2328\n",
      "Epoch [2300], Loss: 0.2414, validation loss: 0.2304\n",
      "Epoch [2400], Loss: 0.2393, validation loss: 0.2283\n",
      "Epoch [2500], Loss: 0.2373, validation loss: 0.2264\n",
      "Epoch [2600], Loss: 0.2355, validation loss: 0.2245\n",
      "Epoch [2700], Loss: 0.2338, validation loss: 0.2230\n",
      "Epoch [2800], Loss: 0.2322, validation loss: 0.2215\n",
      "Epoch [2900], Loss: 0.2308, validation loss: 0.2202\n",
      "Epoch [3000], Loss: 0.2295, validation loss: 0.2189\n",
      "Epoch [3100], Loss: 0.2282, validation loss: 0.2178\n",
      "Epoch [3200], Loss: 0.2270, validation loss: 0.2166\n",
      "Epoch [3300], Loss: 0.2258, validation loss: 0.2156\n",
      "Epoch [3400], Loss: 0.2246, validation loss: 0.2148\n",
      "Epoch [3500], Loss: 0.2233, validation loss: 0.2141\n",
      "Epoch [3600], Loss: 0.2220, validation loss: 0.2135\n",
      "Epoch [3700], Loss: 0.2208, validation loss: 0.2127\n",
      "Epoch [3800], Loss: 0.2196, validation loss: 0.2120\n",
      "Epoch [3900], Loss: 0.2184, validation loss: 0.2110\n",
      "Epoch [4000], Loss: 0.2172, validation loss: 0.2102\n",
      "Epoch [4100], Loss: 0.2161, validation loss: 0.2094\n",
      "Epoch [4200], Loss: 0.2151, validation loss: 0.2086\n",
      "Epoch [4300], Loss: 0.2140, validation loss: 0.2080\n",
      "Epoch [4400], Loss: 0.2130, validation loss: 0.2073\n",
      "Epoch [4500], Loss: 0.2121, validation loss: 0.2066\n",
      "Epoch [4600], Loss: 0.2112, validation loss: 0.2058\n",
      "Epoch [4700], Loss: 0.2103, validation loss: 0.2051\n",
      "Epoch [4800], Loss: 0.2095, validation loss: 0.2044\n",
      "Epoch [4900], Loss: 0.2087, validation loss: 0.2037\n",
      "Epoch [5000], Loss: 0.2080, validation loss: 0.2030\n",
      "Epoch [5100], Loss: 0.2073, validation loss: 0.2024\n",
      "Epoch [5200], Loss: 0.2067, validation loss: 0.2019\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.6054, validation loss: 1.6010\n",
      "Epoch [200], Loss: 0.8768, validation loss: 0.8710\n",
      "Epoch [300], Loss: 0.5528, validation loss: 0.5438\n",
      "Epoch [400], Loss: 0.4354, validation loss: 0.4236\n",
      "Epoch [500], Loss: 0.3792, validation loss: 0.3659\n",
      "Epoch [600], Loss: 0.3468, validation loss: 0.3331\n",
      "Epoch [700], Loss: 0.3255, validation loss: 0.3118\n",
      "Epoch [800], Loss: 0.3102, validation loss: 0.2967\n",
      "Epoch [900], Loss: 0.2986, validation loss: 0.2852\n",
      "Epoch [1000], Loss: 0.2893, validation loss: 0.2762\n",
      "Epoch [1100], Loss: 0.2815, validation loss: 0.2687\n",
      "Epoch [1200], Loss: 0.2749, validation loss: 0.2621\n",
      "Epoch [1300], Loss: 0.2693, validation loss: 0.2563\n",
      "Epoch [1400], Loss: 0.2643, validation loss: 0.2512\n",
      "Epoch [1500], Loss: 0.2597, validation loss: 0.2464\n",
      "Epoch [1600], Loss: 0.2556, validation loss: 0.2420\n",
      "Epoch [1700], Loss: 0.2518, validation loss: 0.2381\n",
      "Epoch [1800], Loss: 0.2484, validation loss: 0.2346\n",
      "Epoch [1900], Loss: 0.2453, validation loss: 0.2313\n",
      "Epoch [2000], Loss: 0.2425, validation loss: 0.2283\n",
      "Epoch [2100], Loss: 0.2399, validation loss: 0.2255\n",
      "Epoch [2200], Loss: 0.2375, validation loss: 0.2229\n",
      "Epoch [2300], Loss: 0.2353, validation loss: 0.2204\n",
      "Epoch [2400], Loss: 0.2333, validation loss: 0.2179\n",
      "Epoch [2500], Loss: 0.2314, validation loss: 0.2155\n",
      "Epoch [2600], Loss: 0.2296, validation loss: 0.2133\n",
      "Epoch [2700], Loss: 0.2280, validation loss: 0.2113\n",
      "Epoch [2800], Loss: 0.2265, validation loss: 0.2094\n",
      "Epoch [2900], Loss: 0.2251, validation loss: 0.2077\n",
      "Epoch [3000], Loss: 0.2238, validation loss: 0.2062\n",
      "Epoch [3100], Loss: 0.2227, validation loss: 0.2047\n",
      "Epoch [3200], Loss: 0.2216, validation loss: 0.2035\n",
      "Epoch [3300], Loss: 0.2206, validation loss: 0.2024\n",
      "Epoch [3400], Loss: 0.2195, validation loss: 0.2016\n",
      "Epoch [3500], Loss: 0.2184, validation loss: 0.2008\n",
      "Epoch [3600], Loss: 0.2174, validation loss: 0.2001\n",
      "Epoch [3700], Loss: 0.2164, validation loss: 0.1994\n",
      "Epoch [3800], Loss: 0.2155, validation loss: 0.1988\n",
      "Epoch [3900], Loss: 0.2146, validation loss: 0.1981\n",
      "Epoch [4000], Loss: 0.2138, validation loss: 0.1977\n",
      "Epoch [4100], Loss: 0.2130, validation loss: 0.1973\n",
      "Epoch [4200], Loss: 0.2123, validation loss: 0.1968\n",
      "Epoch [4300], Loss: 0.2116, validation loss: 0.1963\n",
      "Epoch [4400], Loss: 0.2110, validation loss: 0.1958\n",
      "Epoch [4500], Loss: 0.2104, validation loss: 0.1953\n",
      "Epoch [4600], Loss: 0.2098, validation loss: 0.1949\n",
      "Epoch [4700], Loss: 0.2092, validation loss: 0.1944\n",
      "Epoch [4800], Loss: 0.2086, validation loss: 0.1937\n",
      "Epoch [4900], Loss: 0.2080, validation loss: 0.1931\n",
      "Epoch [5000], Loss: 0.2075, validation loss: 0.1927\n",
      "Epoch [5100], Loss: 0.2069, validation loss: 0.1922\n",
      "Epoch [5200], Loss: 0.2064, validation loss: 0.1919\n",
      "Epoch [5300], Loss: 0.2059, validation loss: 0.1916\n",
      "Epoch [5400], Loss: 0.2054, validation loss: 0.1912\n",
      "Epoch [5500], Loss: 0.2049, validation loss: 0.1908\n",
      "Epoch [5600], Loss: 0.2043, validation loss: 0.1902\n",
      "Epoch [5700], Loss: 0.2037, validation loss: 0.1894\n",
      "Epoch [5800], Loss: 0.2031, validation loss: 0.1887\n",
      "Epoch [5900], Loss: 0.2025, validation loss: 0.1881\n",
      "Epoch [6000], Loss: 0.2020, validation loss: 0.1877\n",
      "Epoch [6100], Loss: 0.2015, validation loss: 0.1872\n",
      "Epoch [6200], Loss: 0.2011, validation loss: 0.1867\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.5912, validation loss: 1.5872\n",
      "Epoch [200], Loss: 0.9253, validation loss: 0.9206\n",
      "Epoch [300], Loss: 0.6080, validation loss: 0.5980\n",
      "Epoch [400], Loss: 0.4650, validation loss: 0.4520\n",
      "Epoch [500], Loss: 0.3955, validation loss: 0.3824\n",
      "Epoch [600], Loss: 0.3579, validation loss: 0.3453\n",
      "Epoch [700], Loss: 0.3348, validation loss: 0.3223\n",
      "Epoch [800], Loss: 0.3188, validation loss: 0.3063\n",
      "Epoch [900], Loss: 0.3067, validation loss: 0.2941\n",
      "Epoch [1000], Loss: 0.2971, validation loss: 0.2844\n",
      "Epoch [1100], Loss: 0.2891, validation loss: 0.2761\n",
      "Epoch [1200], Loss: 0.2822, validation loss: 0.2689\n",
      "Epoch [1300], Loss: 0.2763, validation loss: 0.2625\n",
      "Epoch [1400], Loss: 0.2709, validation loss: 0.2568\n",
      "Epoch [1500], Loss: 0.2660, validation loss: 0.2516\n",
      "Epoch [1600], Loss: 0.2615, validation loss: 0.2471\n",
      "Epoch [1700], Loss: 0.2575, validation loss: 0.2432\n",
      "Epoch [1800], Loss: 0.2538, validation loss: 0.2398\n",
      "Epoch [1900], Loss: 0.2505, validation loss: 0.2367\n",
      "Epoch [2000], Loss: 0.2474, validation loss: 0.2338\n",
      "Epoch [2100], Loss: 0.2445, validation loss: 0.2312\n",
      "Epoch [2200], Loss: 0.2418, validation loss: 0.2287\n",
      "Epoch [2300], Loss: 0.2393, validation loss: 0.2262\n",
      "Epoch [2400], Loss: 0.2369, validation loss: 0.2238\n",
      "Epoch [2500], Loss: 0.2346, validation loss: 0.2217\n",
      "Epoch [2600], Loss: 0.2325, validation loss: 0.2197\n",
      "Epoch [2700], Loss: 0.2305, validation loss: 0.2177\n",
      "Epoch [2800], Loss: 0.2286, validation loss: 0.2160\n",
      "Epoch [2900], Loss: 0.2268, validation loss: 0.2143\n",
      "Epoch [3000], Loss: 0.2253, validation loss: 0.2129\n",
      "Epoch [3100], Loss: 0.2238, validation loss: 0.2116\n",
      "Epoch [3200], Loss: 0.2225, validation loss: 0.2104\n",
      "Epoch [3300], Loss: 0.2213, validation loss: 0.2093\n",
      "Epoch [3400], Loss: 0.2200, validation loss: 0.2081\n",
      "Epoch [3500], Loss: 0.2188, validation loss: 0.2070\n",
      "Epoch [3600], Loss: 0.2176, validation loss: 0.2060\n",
      "Epoch [3700], Loss: 0.2163, validation loss: 0.2052\n",
      "Epoch [3800], Loss: 0.2150, validation loss: 0.2046\n",
      "Epoch [3900], Loss: 0.2139, validation loss: 0.2037\n",
      "Epoch [4000], Loss: 0.2127, validation loss: 0.2030\n",
      "Epoch [4100], Loss: 0.2116, validation loss: 0.2025\n",
      "Epoch [4200], Loss: 0.2106, validation loss: 0.2019\n",
      "Epoch [4300], Loss: 0.2096, validation loss: 0.2012\n",
      "Epoch [4400], Loss: 0.2087, validation loss: 0.2005\n",
      "Epoch [4500], Loss: 0.2078, validation loss: 0.1996\n",
      "Epoch [4600], Loss: 0.2069, validation loss: 0.1986\n",
      "Epoch [4700], Loss: 0.2059, validation loss: 0.1976\n",
      "Epoch [4800], Loss: 0.2048, validation loss: 0.1964\n",
      "Epoch [4900], Loss: 0.2038, validation loss: 0.1952\n",
      "Epoch [5000], Loss: 0.2029, validation loss: 0.1943\n",
      "Epoch [5100], Loss: 0.2021, validation loss: 0.1934\n",
      "Epoch [5200], Loss: 0.2013, validation loss: 0.1927\n",
      "Epoch [5300], Loss: 0.2006, validation loss: 0.1920\n",
      "Epoch [5400], Loss: 0.2000, validation loss: 0.1916\n",
      "Epoch [5500], Loss: 0.1993, validation loss: 0.1911\n",
      "Epoch [5600], Loss: 0.1987, validation loss: 0.1906\n",
      "Epoch [5700], Loss: 0.1982, validation loss: 0.1900\n",
      "Epoch [5800], Loss: 0.1978, validation loss: 0.1895\n",
      "Epoch [5900], Loss: 0.1974, validation loss: 0.1891\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.5057, validation loss: 1.4968\n",
      "Epoch [200], Loss: 0.8178, validation loss: 0.8096\n",
      "Epoch [300], Loss: 0.5393, validation loss: 0.5282\n",
      "Epoch [400], Loss: 0.4238, validation loss: 0.4106\n",
      "Epoch [500], Loss: 0.3700, validation loss: 0.3562\n",
      "Epoch [600], Loss: 0.3408, validation loss: 0.3267\n",
      "Epoch [700], Loss: 0.3225, validation loss: 0.3082\n",
      "Epoch [800], Loss: 0.3099, validation loss: 0.2952\n",
      "Epoch [900], Loss: 0.3005, validation loss: 0.2854\n",
      "Epoch [1000], Loss: 0.2931, validation loss: 0.2777\n",
      "Epoch [1100], Loss: 0.2869, validation loss: 0.2714\n",
      "Epoch [1200], Loss: 0.2816, validation loss: 0.2661\n",
      "Epoch [1300], Loss: 0.2768, validation loss: 0.2611\n",
      "Epoch [1400], Loss: 0.2723, validation loss: 0.2565\n",
      "Epoch [1500], Loss: 0.2679, validation loss: 0.2518\n",
      "Epoch [1600], Loss: 0.2636, validation loss: 0.2474\n",
      "Epoch [1700], Loss: 0.2595, validation loss: 0.2430\n",
      "Epoch [1800], Loss: 0.2555, validation loss: 0.2389\n",
      "Epoch [1900], Loss: 0.2518, validation loss: 0.2350\n",
      "Epoch [2000], Loss: 0.2483, validation loss: 0.2313\n",
      "Epoch [2100], Loss: 0.2451, validation loss: 0.2278\n",
      "Epoch [2200], Loss: 0.2421, validation loss: 0.2245\n",
      "Epoch [2300], Loss: 0.2392, validation loss: 0.2212\n",
      "Epoch [2400], Loss: 0.2364, validation loss: 0.2180\n",
      "Epoch [2500], Loss: 0.2337, validation loss: 0.2154\n",
      "Epoch [2600], Loss: 0.2312, validation loss: 0.2131\n",
      "Epoch [2700], Loss: 0.2288, validation loss: 0.2108\n",
      "Epoch [2800], Loss: 0.2264, validation loss: 0.2086\n",
      "Epoch [2900], Loss: 0.2243, validation loss: 0.2064\n",
      "Epoch [3000], Loss: 0.2221, validation loss: 0.2046\n",
      "Epoch [3100], Loss: 0.2200, validation loss: 0.2030\n",
      "Epoch [3200], Loss: 0.2180, validation loss: 0.2016\n",
      "Epoch [3300], Loss: 0.2161, validation loss: 0.2000\n",
      "Epoch [3400], Loss: 0.2142, validation loss: 0.1985\n",
      "Epoch [3500], Loss: 0.2125, validation loss: 0.1972\n",
      "Epoch [3600], Loss: 0.2109, validation loss: 0.1963\n",
      "Epoch [3700], Loss: 0.2094, validation loss: 0.1956\n",
      "Epoch [3800], Loss: 0.2080, validation loss: 0.1947\n",
      "Epoch [3900], Loss: 0.2066, validation loss: 0.1934\n",
      "Epoch [4000], Loss: 0.2053, validation loss: 0.1923\n",
      "Epoch [4100], Loss: 0.2041, validation loss: 0.1913\n",
      "Epoch [4200], Loss: 0.2030, validation loss: 0.1905\n",
      "Epoch [4300], Loss: 0.2019, validation loss: 0.1898\n",
      "Epoch [4400], Loss: 0.2008, validation loss: 0.1888\n",
      "Epoch [4500], Loss: 0.1998, validation loss: 0.1877\n",
      "Epoch [4600], Loss: 0.1988, validation loss: 0.1869\n",
      "Epoch [4700], Loss: 0.1979, validation loss: 0.1859\n",
      "Epoch [4800], Loss: 0.1971, validation loss: 0.1849\n",
      "Epoch [4900], Loss: 0.1963, validation loss: 0.1838\n",
      "Epoch [5000], Loss: 0.1955, validation loss: 0.1828\n",
      "Epoch [5100], Loss: 0.1948, validation loss: 0.1819\n",
      "Epoch [5200], Loss: 0.1941, validation loss: 0.1810\n",
      "Epoch [5300], Loss: 0.1935, validation loss: 0.1803\n",
      "Epoch [5400], Loss: 0.1929, validation loss: 0.1797\n",
      "Epoch [5500], Loss: 0.1924, validation loss: 0.1792\n",
      "Epoch [5600], Loss: 0.1920, validation loss: 0.1788\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.4300, validation loss: 1.4304\n",
      "Epoch [200], Loss: 0.7525, validation loss: 0.7441\n",
      "Epoch [300], Loss: 0.4985, validation loss: 0.4868\n",
      "Epoch [400], Loss: 0.4026, validation loss: 0.3895\n",
      "Epoch [500], Loss: 0.3572, validation loss: 0.3428\n",
      "Epoch [600], Loss: 0.3310, validation loss: 0.3156\n",
      "Epoch [700], Loss: 0.3136, validation loss: 0.2973\n",
      "Epoch [800], Loss: 0.3012, validation loss: 0.2839\n",
      "Epoch [900], Loss: 0.2914, validation loss: 0.2731\n",
      "Epoch [1000], Loss: 0.2834, validation loss: 0.2641\n",
      "Epoch [1100], Loss: 0.2764, validation loss: 0.2565\n",
      "Epoch [1200], Loss: 0.2701, validation loss: 0.2500\n",
      "Epoch [1300], Loss: 0.2644, validation loss: 0.2441\n",
      "Epoch [1400], Loss: 0.2592, validation loss: 0.2389\n",
      "Epoch [1500], Loss: 0.2545, validation loss: 0.2343\n",
      "Epoch [1600], Loss: 0.2501, validation loss: 0.2303\n",
      "Epoch [1700], Loss: 0.2462, validation loss: 0.2267\n",
      "Epoch [1800], Loss: 0.2426, validation loss: 0.2236\n",
      "Epoch [1900], Loss: 0.2393, validation loss: 0.2208\n",
      "Epoch [2000], Loss: 0.2362, validation loss: 0.2181\n",
      "Epoch [2100], Loss: 0.2333, validation loss: 0.2154\n",
      "Epoch [2200], Loss: 0.2305, validation loss: 0.2127\n",
      "Epoch [2300], Loss: 0.2279, validation loss: 0.2100\n",
      "Epoch [2400], Loss: 0.2253, validation loss: 0.2074\n",
      "Epoch [2500], Loss: 0.2229, validation loss: 0.2050\n",
      "Epoch [2600], Loss: 0.2207, validation loss: 0.2027\n",
      "Epoch [2700], Loss: 0.2186, validation loss: 0.2005\n",
      "Epoch [2800], Loss: 0.2166, validation loss: 0.1986\n",
      "Epoch [2900], Loss: 0.2147, validation loss: 0.1968\n",
      "Epoch [3000], Loss: 0.2129, validation loss: 0.1951\n",
      "Epoch [3100], Loss: 0.2112, validation loss: 0.1933\n",
      "Epoch [3200], Loss: 0.2097, validation loss: 0.1919\n",
      "Epoch [3300], Loss: 0.2082, validation loss: 0.1903\n",
      "Epoch [3400], Loss: 0.2069, validation loss: 0.1889\n",
      "Epoch [3500], Loss: 0.2055, validation loss: 0.1875\n",
      "Epoch [3600], Loss: 0.2043, validation loss: 0.1862\n",
      "Epoch [3700], Loss: 0.2032, validation loss: 0.1853\n",
      "Epoch [3800], Loss: 0.2021, validation loss: 0.1844\n",
      "Epoch [3900], Loss: 0.2012, validation loss: 0.1837\n",
      "Epoch [4000], Loss: 0.2002, validation loss: 0.1829\n",
      "Epoch [4100], Loss: 0.1992, validation loss: 0.1822\n",
      "Epoch [4200], Loss: 0.1982, validation loss: 0.1812\n",
      "Epoch [4300], Loss: 0.1972, validation loss: 0.1801\n",
      "Epoch [4400], Loss: 0.1962, validation loss: 0.1792\n",
      "Epoch [4500], Loss: 0.1952, validation loss: 0.1784\n",
      "Epoch [4600], Loss: 0.1944, validation loss: 0.1777\n",
      "Epoch [4700], Loss: 0.1937, validation loss: 0.1772\n",
      "Epoch [4800], Loss: 0.1930, validation loss: 0.1765\n",
      "Epoch [4900], Loss: 0.1923, validation loss: 0.1757\n",
      "Epoch [5000], Loss: 0.1918, validation loss: 0.1750\n",
      "Epoch [5100], Loss: 0.1911, validation loss: 0.1742\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.5531, validation loss: 1.5509\n",
      "Epoch [200], Loss: 0.8142, validation loss: 0.8069\n",
      "Epoch [300], Loss: 0.5342, validation loss: 0.5213\n",
      "Epoch [400], Loss: 0.4228, validation loss: 0.4071\n",
      "Epoch [500], Loss: 0.3690, validation loss: 0.3526\n",
      "Epoch [600], Loss: 0.3385, validation loss: 0.3220\n",
      "Epoch [700], Loss: 0.3187, validation loss: 0.3024\n",
      "Epoch [800], Loss: 0.3044, validation loss: 0.2882\n",
      "Epoch [900], Loss: 0.2934, validation loss: 0.2771\n",
      "Epoch [1000], Loss: 0.2844, validation loss: 0.2680\n",
      "Epoch [1100], Loss: 0.2770, validation loss: 0.2601\n",
      "Epoch [1200], Loss: 0.2706, validation loss: 0.2533\n",
      "Epoch [1300], Loss: 0.2651, validation loss: 0.2474\n",
      "Epoch [1400], Loss: 0.2602, validation loss: 0.2422\n",
      "Epoch [1500], Loss: 0.2559, validation loss: 0.2377\n",
      "Epoch [1600], Loss: 0.2521, validation loss: 0.2337\n",
      "Epoch [1700], Loss: 0.2487, validation loss: 0.2302\n",
      "Epoch [1800], Loss: 0.2455, validation loss: 0.2270\n",
      "Epoch [1900], Loss: 0.2426, validation loss: 0.2242\n",
      "Epoch [2000], Loss: 0.2398, validation loss: 0.2215\n",
      "Epoch [2100], Loss: 0.2372, validation loss: 0.2187\n",
      "Epoch [2200], Loss: 0.2345, validation loss: 0.2159\n",
      "Epoch [2300], Loss: 0.2319, validation loss: 0.2131\n",
      "Epoch [2400], Loss: 0.2293, validation loss: 0.2103\n",
      "Epoch [2500], Loss: 0.2267, validation loss: 0.2077\n",
      "Epoch [2600], Loss: 0.2244, validation loss: 0.2050\n",
      "Epoch [2700], Loss: 0.2222, validation loss: 0.2024\n",
      "Epoch [2800], Loss: 0.2201, validation loss: 0.2003\n",
      "Epoch [2900], Loss: 0.2181, validation loss: 0.1986\n",
      "Epoch [3000], Loss: 0.2162, validation loss: 0.1970\n",
      "Epoch [3100], Loss: 0.2143, validation loss: 0.1956\n",
      "Epoch [3200], Loss: 0.2125, validation loss: 0.1941\n",
      "Epoch [3300], Loss: 0.2107, validation loss: 0.1926\n",
      "Epoch [3400], Loss: 0.2092, validation loss: 0.1913\n",
      "Epoch [3500], Loss: 0.2077, validation loss: 0.1901\n",
      "Epoch [3600], Loss: 0.2063, validation loss: 0.1891\n",
      "Epoch [3700], Loss: 0.2050, validation loss: 0.1881\n",
      "Epoch [3800], Loss: 0.2038, validation loss: 0.1872\n",
      "Epoch [3900], Loss: 0.2027, validation loss: 0.1865\n",
      "Epoch [4000], Loss: 0.2016, validation loss: 0.1859\n",
      "Epoch [4100], Loss: 0.2006, validation loss: 0.1854\n",
      "Epoch [4200], Loss: 0.1997, validation loss: 0.1850\n",
      "Epoch [4300], Loss: 0.1989, validation loss: 0.1846\n",
      "Epoch [4400], Loss: 0.1979, validation loss: 0.1839\n",
      "Epoch [4500], Loss: 0.1970, validation loss: 0.1831\n",
      "Epoch [4600], Loss: 0.1960, validation loss: 0.1823\n",
      "Epoch [4700], Loss: 0.1951, validation loss: 0.1815\n",
      "Epoch [4800], Loss: 0.1941, validation loss: 0.1806\n",
      "Epoch [4900], Loss: 0.1932, validation loss: 0.1797\n",
      "Epoch [5000], Loss: 0.1924, validation loss: 0.1786\n",
      "Epoch [5100], Loss: 0.1916, validation loss: 0.1778\n",
      "Epoch [5200], Loss: 0.1908, validation loss: 0.1769\n",
      "Epoch [5300], Loss: 0.1900, validation loss: 0.1760\n",
      "Epoch [5400], Loss: 0.1893, validation loss: 0.1753\n",
      "Epoch [5500], Loss: 0.1887, validation loss: 0.1747\n",
      "Epoch [5600], Loss: 0.1883, validation loss: 0.1743\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.4100, validation loss: 1.4040\n",
      "Epoch [200], Loss: 0.7504, validation loss: 0.7391\n",
      "Epoch [300], Loss: 0.5060, validation loss: 0.4904\n",
      "Epoch [400], Loss: 0.4046, validation loss: 0.3868\n",
      "Epoch [500], Loss: 0.3553, validation loss: 0.3362\n",
      "Epoch [600], Loss: 0.3267, validation loss: 0.3069\n",
      "Epoch [700], Loss: 0.3078, validation loss: 0.2874\n",
      "Epoch [800], Loss: 0.2940, validation loss: 0.2730\n",
      "Epoch [900], Loss: 0.2832, validation loss: 0.2617\n",
      "Epoch [1000], Loss: 0.2743, validation loss: 0.2524\n",
      "Epoch [1100], Loss: 0.2669, validation loss: 0.2446\n",
      "Epoch [1200], Loss: 0.2605, validation loss: 0.2377\n",
      "Epoch [1300], Loss: 0.2547, validation loss: 0.2314\n",
      "Epoch [1400], Loss: 0.2493, validation loss: 0.2258\n",
      "Epoch [1500], Loss: 0.2444, validation loss: 0.2207\n",
      "Epoch [1600], Loss: 0.2398, validation loss: 0.2159\n",
      "Epoch [1700], Loss: 0.2355, validation loss: 0.2114\n",
      "Epoch [1800], Loss: 0.2316, validation loss: 0.2073\n",
      "Epoch [1900], Loss: 0.2281, validation loss: 0.2038\n",
      "Epoch [2000], Loss: 0.2248, validation loss: 0.2007\n",
      "Epoch [2100], Loss: 0.2218, validation loss: 0.1979\n",
      "Epoch [2200], Loss: 0.2190, validation loss: 0.1954\n",
      "Epoch [2300], Loss: 0.2165, validation loss: 0.1931\n",
      "Epoch [2400], Loss: 0.2142, validation loss: 0.1911\n",
      "Epoch [2500], Loss: 0.2121, validation loss: 0.1892\n",
      "Epoch [2600], Loss: 0.2102, validation loss: 0.1874\n",
      "Epoch [2700], Loss: 0.2085, validation loss: 0.1858\n",
      "Epoch [2800], Loss: 0.2069, validation loss: 0.1842\n",
      "Epoch [2900], Loss: 0.2055, validation loss: 0.1828\n",
      "Epoch [3000], Loss: 0.2041, validation loss: 0.1817\n",
      "Epoch [3100], Loss: 0.2029, validation loss: 0.1807\n",
      "Epoch [3200], Loss: 0.2018, validation loss: 0.1796\n",
      "Epoch [3300], Loss: 0.2008, validation loss: 0.1784\n",
      "Epoch [3400], Loss: 0.1998, validation loss: 0.1774\n",
      "Epoch [3500], Loss: 0.1989, validation loss: 0.1768\n",
      "Epoch [3600], Loss: 0.1980, validation loss: 0.1760\n",
      "Epoch [3700], Loss: 0.1972, validation loss: 0.1752\n",
      "Epoch [3800], Loss: 0.1963, validation loss: 0.1742\n",
      "Epoch [3900], Loss: 0.1955, validation loss: 0.1733\n",
      "Epoch [4000], Loss: 0.1947, validation loss: 0.1724\n",
      "Epoch [4100], Loss: 0.1940, validation loss: 0.1717\n",
      "Epoch [4200], Loss: 0.1933, validation loss: 0.1710\n",
      "Epoch [4300], Loss: 0.1926, validation loss: 0.1703\n",
      "Epoch [4400], Loss: 0.1919, validation loss: 0.1696\n",
      "Epoch [4500], Loss: 0.1912, validation loss: 0.1688\n",
      "Epoch [4600], Loss: 0.1905, validation loss: 0.1682\n",
      "Epoch [4700], Loss: 0.1898, validation loss: 0.1677\n",
      "Epoch [4800], Loss: 0.1892, validation loss: 0.1672\n",
      "Epoch [4900], Loss: 0.1885, validation loss: 0.1668\n",
      "Epoch [5000], Loss: 0.1879, validation loss: 0.1664\n",
      "Epoch [5100], Loss: 0.1872, validation loss: 0.1659\n",
      "Epoch [5200], Loss: 0.1866, validation loss: 0.1653\n",
      "Epoch [5300], Loss: 0.1860, validation loss: 0.1650\n",
      "Epoch [5400], Loss: 0.1854, validation loss: 0.1647\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.3392, validation loss: 1.3350\n",
      "Epoch [200], Loss: 0.7226, validation loss: 0.7130\n",
      "Epoch [300], Loss: 0.4946, validation loss: 0.4820\n",
      "Epoch [400], Loss: 0.4030, validation loss: 0.3884\n",
      "Epoch [500], Loss: 0.3585, validation loss: 0.3430\n",
      "Epoch [600], Loss: 0.3324, validation loss: 0.3161\n",
      "Epoch [700], Loss: 0.3147, validation loss: 0.2974\n",
      "Epoch [800], Loss: 0.3015, validation loss: 0.2831\n",
      "Epoch [900], Loss: 0.2910, validation loss: 0.2716\n",
      "Epoch [1000], Loss: 0.2821, validation loss: 0.2616\n",
      "Epoch [1100], Loss: 0.2742, validation loss: 0.2527\n",
      "Epoch [1200], Loss: 0.2674, validation loss: 0.2448\n",
      "Epoch [1300], Loss: 0.2613, validation loss: 0.2379\n",
      "Epoch [1400], Loss: 0.2560, validation loss: 0.2317\n",
      "Epoch [1500], Loss: 0.2511, validation loss: 0.2262\n",
      "Epoch [1600], Loss: 0.2469, validation loss: 0.2213\n",
      "Epoch [1700], Loss: 0.2431, validation loss: 0.2169\n",
      "Epoch [1800], Loss: 0.2397, validation loss: 0.2131\n",
      "Epoch [1900], Loss: 0.2365, validation loss: 0.2095\n",
      "Epoch [2000], Loss: 0.2336, validation loss: 0.2063\n",
      "Epoch [2100], Loss: 0.2309, validation loss: 0.2034\n",
      "Epoch [2200], Loss: 0.2284, validation loss: 0.2007\n",
      "Epoch [2300], Loss: 0.2259, validation loss: 0.1981\n",
      "Epoch [2400], Loss: 0.2236, validation loss: 0.1958\n",
      "Epoch [2500], Loss: 0.2213, validation loss: 0.1935\n",
      "Epoch [2600], Loss: 0.2192, validation loss: 0.1915\n",
      "Epoch [2700], Loss: 0.2173, validation loss: 0.1894\n",
      "Epoch [2800], Loss: 0.2154, validation loss: 0.1877\n",
      "Epoch [2900], Loss: 0.2137, validation loss: 0.1859\n",
      "Epoch [3000], Loss: 0.2121, validation loss: 0.1843\n",
      "Epoch [3100], Loss: 0.2104, validation loss: 0.1827\n",
      "Epoch [3200], Loss: 0.2089, validation loss: 0.1811\n",
      "Epoch [3300], Loss: 0.2075, validation loss: 0.1796\n",
      "Epoch [3400], Loss: 0.2061, validation loss: 0.1784\n",
      "Epoch [3500], Loss: 0.2049, validation loss: 0.1774\n",
      "Epoch [3600], Loss: 0.2036, validation loss: 0.1764\n",
      "Epoch [3700], Loss: 0.2023, validation loss: 0.1756\n",
      "Epoch [3800], Loss: 0.2012, validation loss: 0.1748\n",
      "Epoch [3900], Loss: 0.2000, validation loss: 0.1740\n",
      "Epoch [4000], Loss: 0.1988, validation loss: 0.1731\n",
      "Epoch [4100], Loss: 0.1976, validation loss: 0.1723\n",
      "Epoch [4200], Loss: 0.1965, validation loss: 0.1717\n",
      "Epoch [4300], Loss: 0.1953, validation loss: 0.1713\n",
      "Epoch [4400], Loss: 0.1942, validation loss: 0.1708\n",
      "Epoch [4500], Loss: 0.1931, validation loss: 0.1702\n",
      "Epoch [4600], Loss: 0.1920, validation loss: 0.1698\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.3894, validation loss: 1.3803\n",
      "Epoch [200], Loss: 0.7269, validation loss: 0.7177\n",
      "Epoch [300], Loss: 0.4848, validation loss: 0.4726\n",
      "Epoch [400], Loss: 0.3918, validation loss: 0.3775\n",
      "Epoch [500], Loss: 0.3487, validation loss: 0.3329\n",
      "Epoch [600], Loss: 0.3241, validation loss: 0.3069\n",
      "Epoch [700], Loss: 0.3073, validation loss: 0.2890\n",
      "Epoch [800], Loss: 0.2947, validation loss: 0.2757\n",
      "Epoch [900], Loss: 0.2845, validation loss: 0.2652\n",
      "Epoch [1000], Loss: 0.2759, validation loss: 0.2559\n",
      "Epoch [1100], Loss: 0.2685, validation loss: 0.2478\n",
      "Epoch [1200], Loss: 0.2619, validation loss: 0.2406\n",
      "Epoch [1300], Loss: 0.2560, validation loss: 0.2344\n",
      "Epoch [1400], Loss: 0.2506, validation loss: 0.2285\n",
      "Epoch [1500], Loss: 0.2457, validation loss: 0.2231\n",
      "Epoch [1600], Loss: 0.2411, validation loss: 0.2183\n",
      "Epoch [1700], Loss: 0.2370, validation loss: 0.2139\n",
      "Epoch [1800], Loss: 0.2333, validation loss: 0.2098\n",
      "Epoch [1900], Loss: 0.2299, validation loss: 0.2062\n",
      "Epoch [2000], Loss: 0.2268, validation loss: 0.2030\n",
      "Epoch [2100], Loss: 0.2241, validation loss: 0.2004\n",
      "Epoch [2200], Loss: 0.2216, validation loss: 0.1981\n",
      "Epoch [2300], Loss: 0.2193, validation loss: 0.1961\n",
      "Epoch [2400], Loss: 0.2171, validation loss: 0.1942\n",
      "Epoch [2500], Loss: 0.2151, validation loss: 0.1923\n",
      "Epoch [2600], Loss: 0.2132, validation loss: 0.1906\n",
      "Epoch [2700], Loss: 0.2114, validation loss: 0.1889\n",
      "Epoch [2800], Loss: 0.2097, validation loss: 0.1873\n",
      "Epoch [2900], Loss: 0.2081, validation loss: 0.1858\n",
      "Epoch [3000], Loss: 0.2065, validation loss: 0.1844\n",
      "Epoch [3100], Loss: 0.2050, validation loss: 0.1831\n",
      "Epoch [3200], Loss: 0.2034, validation loss: 0.1820\n",
      "Epoch [3300], Loss: 0.2019, validation loss: 0.1807\n",
      "Epoch [3400], Loss: 0.2005, validation loss: 0.1797\n",
      "Epoch [3500], Loss: 0.1992, validation loss: 0.1788\n",
      "Epoch [3600], Loss: 0.1981, validation loss: 0.1779\n",
      "Epoch [3700], Loss: 0.1969, validation loss: 0.1772\n",
      "Epoch [3800], Loss: 0.1958, validation loss: 0.1766\n",
      "Epoch [3900], Loss: 0.1948, validation loss: 0.1761\n",
      "Epoch [4000], Loss: 0.1939, validation loss: 0.1757\n",
      "Epoch [4100], Loss: 0.1930, validation loss: 0.1753\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.3315, validation loss: 1.3302\n",
      "Epoch [200], Loss: 0.6973, validation loss: 0.6923\n",
      "Epoch [300], Loss: 0.4758, validation loss: 0.4660\n",
      "Epoch [400], Loss: 0.3905, validation loss: 0.3781\n",
      "Epoch [500], Loss: 0.3481, validation loss: 0.3340\n",
      "Epoch [600], Loss: 0.3226, validation loss: 0.3073\n",
      "Epoch [700], Loss: 0.3053, validation loss: 0.2889\n",
      "Epoch [800], Loss: 0.2925, validation loss: 0.2753\n",
      "Epoch [900], Loss: 0.2824, validation loss: 0.2645\n",
      "Epoch [1000], Loss: 0.2740, validation loss: 0.2557\n",
      "Epoch [1100], Loss: 0.2668, validation loss: 0.2482\n",
      "Epoch [1200], Loss: 0.2606, validation loss: 0.2415\n",
      "Epoch [1300], Loss: 0.2552, validation loss: 0.2356\n",
      "Epoch [1400], Loss: 0.2503, validation loss: 0.2303\n",
      "Epoch [1500], Loss: 0.2458, validation loss: 0.2254\n",
      "Epoch [1600], Loss: 0.2417, validation loss: 0.2208\n",
      "Epoch [1700], Loss: 0.2380, validation loss: 0.2169\n",
      "Epoch [1800], Loss: 0.2346, validation loss: 0.2135\n",
      "Epoch [1900], Loss: 0.2313, validation loss: 0.2104\n",
      "Epoch [2000], Loss: 0.2283, validation loss: 0.2076\n",
      "Epoch [2100], Loss: 0.2255, validation loss: 0.2050\n",
      "Epoch [2200], Loss: 0.2229, validation loss: 0.2027\n",
      "Epoch [2300], Loss: 0.2204, validation loss: 0.2006\n",
      "Epoch [2400], Loss: 0.2181, validation loss: 0.1985\n",
      "Epoch [2500], Loss: 0.2160, validation loss: 0.1964\n",
      "Epoch [2600], Loss: 0.2141, validation loss: 0.1946\n",
      "Epoch [2700], Loss: 0.2123, validation loss: 0.1927\n",
      "Epoch [2800], Loss: 0.2104, validation loss: 0.1909\n",
      "Epoch [2900], Loss: 0.2085, validation loss: 0.1890\n",
      "Epoch [3000], Loss: 0.2066, validation loss: 0.1873\n",
      "Epoch [3100], Loss: 0.2048, validation loss: 0.1857\n",
      "Epoch [3200], Loss: 0.2030, validation loss: 0.1842\n",
      "Epoch [3300], Loss: 0.2012, validation loss: 0.1829\n",
      "Epoch [3400], Loss: 0.1996, validation loss: 0.1815\n",
      "Epoch [3500], Loss: 0.1980, validation loss: 0.1802\n",
      "Epoch [3600], Loss: 0.1966, validation loss: 0.1789\n",
      "Epoch [3700], Loss: 0.1953, validation loss: 0.1774\n",
      "Epoch [3800], Loss: 0.1940, validation loss: 0.1760\n",
      "Epoch [3900], Loss: 0.1928, validation loss: 0.1745\n",
      "Epoch [4000], Loss: 0.1915, validation loss: 0.1733\n",
      "Epoch [4100], Loss: 0.1904, validation loss: 0.1720\n",
      "Epoch [4200], Loss: 0.1893, validation loss: 0.1709\n",
      "Epoch [4300], Loss: 0.1883, validation loss: 0.1697\n",
      "Epoch [4400], Loss: 0.1873, validation loss: 0.1687\n",
      "Epoch [4500], Loss: 0.1864, validation loss: 0.1678\n",
      "Epoch [4600], Loss: 0.1856, validation loss: 0.1669\n",
      "Epoch [4700], Loss: 0.1848, validation loss: 0.1663\n",
      "Epoch [4800], Loss: 0.1841, validation loss: 0.1658\n",
      "Epoch [4900], Loss: 0.1834, validation loss: 0.1654\n",
      "Epoch [5000], Loss: 0.1826, validation loss: 0.1648\n",
      "Epoch [5100], Loss: 0.1820, validation loss: 0.1646\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.2568, validation loss: 1.2544\n",
      "Epoch [200], Loss: 0.6771, validation loss: 0.6697\n",
      "Epoch [300], Loss: 0.4698, validation loss: 0.4582\n",
      "Epoch [400], Loss: 0.3867, validation loss: 0.3715\n",
      "Epoch [500], Loss: 0.3462, validation loss: 0.3284\n",
      "Epoch [600], Loss: 0.3219, validation loss: 0.3025\n",
      "Epoch [700], Loss: 0.3050, validation loss: 0.2845\n",
      "Epoch [800], Loss: 0.2923, validation loss: 0.2709\n",
      "Epoch [900], Loss: 0.2820, validation loss: 0.2602\n",
      "Epoch [1000], Loss: 0.2734, validation loss: 0.2514\n",
      "Epoch [1100], Loss: 0.2658, validation loss: 0.2439\n",
      "Epoch [1200], Loss: 0.2589, validation loss: 0.2372\n",
      "Epoch [1300], Loss: 0.2529, validation loss: 0.2315\n",
      "Epoch [1400], Loss: 0.2474, validation loss: 0.2265\n",
      "Epoch [1500], Loss: 0.2425, validation loss: 0.2220\n",
      "Epoch [1600], Loss: 0.2380, validation loss: 0.2177\n",
      "Epoch [1700], Loss: 0.2338, validation loss: 0.2134\n",
      "Epoch [1800], Loss: 0.2300, validation loss: 0.2094\n",
      "Epoch [1900], Loss: 0.2265, validation loss: 0.2056\n",
      "Epoch [2000], Loss: 0.2232, validation loss: 0.2021\n",
      "Epoch [2100], Loss: 0.2202, validation loss: 0.1987\n",
      "Epoch [2200], Loss: 0.2175, validation loss: 0.1955\n",
      "Epoch [2300], Loss: 0.2149, validation loss: 0.1927\n",
      "Epoch [2400], Loss: 0.2125, validation loss: 0.1901\n",
      "Epoch [2500], Loss: 0.2102, validation loss: 0.1878\n",
      "Epoch [2600], Loss: 0.2081, validation loss: 0.1856\n",
      "Epoch [2700], Loss: 0.2062, validation loss: 0.1838\n",
      "Epoch [2800], Loss: 0.2045, validation loss: 0.1821\n",
      "Epoch [2900], Loss: 0.2030, validation loss: 0.1806\n",
      "Epoch [3000], Loss: 0.2015, validation loss: 0.1790\n",
      "Epoch [3100], Loss: 0.2002, validation loss: 0.1779\n",
      "Epoch [3200], Loss: 0.1990, validation loss: 0.1769\n",
      "Epoch [3300], Loss: 0.1978, validation loss: 0.1759\n",
      "Epoch [3400], Loss: 0.1966, validation loss: 0.1750\n",
      "Epoch [3500], Loss: 0.1955, validation loss: 0.1741\n",
      "Epoch [3600], Loss: 0.1944, validation loss: 0.1734\n",
      "Epoch [3700], Loss: 0.1933, validation loss: 0.1724\n",
      "Epoch [3800], Loss: 0.1922, validation loss: 0.1712\n",
      "Epoch [3900], Loss: 0.1910, validation loss: 0.1703\n",
      "Epoch [4000], Loss: 0.1898, validation loss: 0.1691\n",
      "Epoch [4100], Loss: 0.1887, validation loss: 0.1678\n",
      "Epoch [4200], Loss: 0.1876, validation loss: 0.1666\n",
      "Epoch [4300], Loss: 0.1866, validation loss: 0.1659\n",
      "Epoch [4400], Loss: 0.1857, validation loss: 0.1653\n",
      "Epoch [4500], Loss: 0.1849, validation loss: 0.1649\n",
      "Epoch [4600], Loss: 0.1841, validation loss: 0.1644\n",
      "Epoch [4700], Loss: 0.1835, validation loss: 0.1637\n",
      "Epoch [4800], Loss: 0.1828, validation loss: 0.1632\n",
      "Epoch [4900], Loss: 0.1822, validation loss: 0.1628\n",
      "Epoch [5000], Loss: 0.1817, validation loss: 0.1624\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.3536, validation loss: 1.3458\n",
      "Epoch [200], Loss: 0.6775, validation loss: 0.6627\n",
      "Epoch [300], Loss: 0.4667, validation loss: 0.4495\n",
      "Epoch [400], Loss: 0.3851, validation loss: 0.3665\n",
      "Epoch [500], Loss: 0.3441, validation loss: 0.3251\n",
      "Epoch [600], Loss: 0.3195, validation loss: 0.3009\n",
      "Epoch [700], Loss: 0.3029, validation loss: 0.2848\n",
      "Epoch [800], Loss: 0.2907, validation loss: 0.2731\n",
      "Epoch [900], Loss: 0.2808, validation loss: 0.2638\n",
      "Epoch [1000], Loss: 0.2727, validation loss: 0.2560\n",
      "Epoch [1100], Loss: 0.2657, validation loss: 0.2489\n",
      "Epoch [1200], Loss: 0.2595, validation loss: 0.2424\n",
      "Epoch [1300], Loss: 0.2537, validation loss: 0.2361\n",
      "Epoch [1400], Loss: 0.2481, validation loss: 0.2298\n",
      "Epoch [1500], Loss: 0.2431, validation loss: 0.2239\n",
      "Epoch [1600], Loss: 0.2387, validation loss: 0.2183\n",
      "Epoch [1700], Loss: 0.2347, validation loss: 0.2132\n",
      "Epoch [1800], Loss: 0.2309, validation loss: 0.2086\n",
      "Epoch [1900], Loss: 0.2274, validation loss: 0.2045\n",
      "Epoch [2000], Loss: 0.2242, validation loss: 0.2008\n",
      "Epoch [2100], Loss: 0.2212, validation loss: 0.1974\n",
      "Epoch [2200], Loss: 0.2184, validation loss: 0.1945\n",
      "Epoch [2300], Loss: 0.2157, validation loss: 0.1919\n",
      "Epoch [2400], Loss: 0.2131, validation loss: 0.1895\n",
      "Epoch [2500], Loss: 0.2106, validation loss: 0.1873\n",
      "Epoch [2600], Loss: 0.2082, validation loss: 0.1850\n",
      "Epoch [2700], Loss: 0.2058, validation loss: 0.1828\n",
      "Epoch [2800], Loss: 0.2036, validation loss: 0.1807\n",
      "Epoch [2900], Loss: 0.2016, validation loss: 0.1786\n",
      "Epoch [3000], Loss: 0.1997, validation loss: 0.1767\n",
      "Epoch [3100], Loss: 0.1979, validation loss: 0.1751\n",
      "Epoch [3200], Loss: 0.1963, validation loss: 0.1735\n",
      "Epoch [3300], Loss: 0.1946, validation loss: 0.1721\n",
      "Epoch [3400], Loss: 0.1931, validation loss: 0.1708\n",
      "Epoch [3500], Loss: 0.1916, validation loss: 0.1697\n",
      "Epoch [3600], Loss: 0.1902, validation loss: 0.1686\n",
      "Epoch [3700], Loss: 0.1890, validation loss: 0.1674\n",
      "Epoch [3800], Loss: 0.1878, validation loss: 0.1662\n",
      "Epoch [3900], Loss: 0.1867, validation loss: 0.1652\n",
      "Epoch [4000], Loss: 0.1857, validation loss: 0.1640\n",
      "Epoch [4100], Loss: 0.1847, validation loss: 0.1632\n",
      "Epoch [4200], Loss: 0.1837, validation loss: 0.1625\n",
      "Epoch [4300], Loss: 0.1828, validation loss: 0.1617\n",
      "Epoch [4400], Loss: 0.1820, validation loss: 0.1610\n",
      "Epoch [4500], Loss: 0.1813, validation loss: 0.1603\n",
      "Epoch [4600], Loss: 0.1805, validation loss: 0.1596\n",
      "Epoch [4700], Loss: 0.1798, validation loss: 0.1590\n",
      "Epoch [4800], Loss: 0.1792, validation loss: 0.1587\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.3243, validation loss: 1.3192\n",
      "Epoch [200], Loss: 0.6578, validation loss: 0.6475\n",
      "Epoch [300], Loss: 0.4535, validation loss: 0.4402\n",
      "Epoch [400], Loss: 0.3767, validation loss: 0.3611\n",
      "Epoch [500], Loss: 0.3389, validation loss: 0.3217\n",
      "Epoch [600], Loss: 0.3163, validation loss: 0.2986\n",
      "Epoch [700], Loss: 0.3010, validation loss: 0.2828\n",
      "Epoch [800], Loss: 0.2895, validation loss: 0.2707\n",
      "Epoch [900], Loss: 0.2803, validation loss: 0.2609\n",
      "Epoch [1000], Loss: 0.2727, validation loss: 0.2525\n",
      "Epoch [1100], Loss: 0.2659, validation loss: 0.2452\n",
      "Epoch [1200], Loss: 0.2599, validation loss: 0.2386\n",
      "Epoch [1300], Loss: 0.2544, validation loss: 0.2325\n",
      "Epoch [1400], Loss: 0.2491, validation loss: 0.2270\n",
      "Epoch [1500], Loss: 0.2440, validation loss: 0.2218\n",
      "Epoch [1600], Loss: 0.2392, validation loss: 0.2167\n",
      "Epoch [1700], Loss: 0.2345, validation loss: 0.2118\n",
      "Epoch [1800], Loss: 0.2301, validation loss: 0.2073\n",
      "Epoch [1900], Loss: 0.2259, validation loss: 0.2032\n",
      "Epoch [2000], Loss: 0.2219, validation loss: 0.1994\n",
      "Epoch [2100], Loss: 0.2182, validation loss: 0.1958\n",
      "Epoch [2200], Loss: 0.2148, validation loss: 0.1926\n",
      "Epoch [2300], Loss: 0.2116, validation loss: 0.1898\n",
      "Epoch [2400], Loss: 0.2086, validation loss: 0.1870\n",
      "Epoch [2500], Loss: 0.2059, validation loss: 0.1843\n",
      "Epoch [2600], Loss: 0.2035, validation loss: 0.1818\n",
      "Epoch [2700], Loss: 0.2013, validation loss: 0.1795\n",
      "Epoch [2800], Loss: 0.1992, validation loss: 0.1771\n",
      "Epoch [2900], Loss: 0.1972, validation loss: 0.1751\n",
      "Epoch [3000], Loss: 0.1954, validation loss: 0.1734\n",
      "Epoch [3100], Loss: 0.1937, validation loss: 0.1716\n",
      "Epoch [3200], Loss: 0.1923, validation loss: 0.1701\n",
      "Epoch [3300], Loss: 0.1909, validation loss: 0.1688\n",
      "Epoch [3400], Loss: 0.1897, validation loss: 0.1675\n",
      "Epoch [3500], Loss: 0.1885, validation loss: 0.1663\n",
      "Epoch [3600], Loss: 0.1874, validation loss: 0.1652\n",
      "Epoch [3700], Loss: 0.1863, validation loss: 0.1641\n",
      "Epoch [3800], Loss: 0.1854, validation loss: 0.1632\n",
      "Epoch [3900], Loss: 0.1845, validation loss: 0.1625\n",
      "Epoch [4000], Loss: 0.1837, validation loss: 0.1619\n",
      "Epoch [4100], Loss: 0.1829, validation loss: 0.1612\n",
      "Epoch [4200], Loss: 0.1821, validation loss: 0.1604\n",
      "Epoch [4300], Loss: 0.1814, validation loss: 0.1596\n",
      "Epoch [4400], Loss: 0.1807, validation loss: 0.1586\n",
      "Epoch [4500], Loss: 0.1800, validation loss: 0.1576\n",
      "Epoch [4600], Loss: 0.1793, validation loss: 0.1567\n",
      "Epoch [4700], Loss: 0.1786, validation loss: 0.1557\n",
      "Epoch [4800], Loss: 0.1779, validation loss: 0.1551\n",
      "Epoch [4900], Loss: 0.1773, validation loss: 0.1544\n",
      "Epoch [5000], Loss: 0.1766, validation loss: 0.1536\n",
      "Epoch [5100], Loss: 0.1760, validation loss: 0.1528\n",
      "Epoch [5200], Loss: 0.1754, validation loss: 0.1519\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.3075, validation loss: 1.3003\n",
      "Epoch [200], Loss: 0.6510, validation loss: 0.6376\n",
      "Epoch [300], Loss: 0.4506, validation loss: 0.4332\n",
      "Epoch [400], Loss: 0.3770, validation loss: 0.3578\n",
      "Epoch [500], Loss: 0.3405, validation loss: 0.3202\n",
      "Epoch [600], Loss: 0.3185, validation loss: 0.2975\n",
      "Epoch [700], Loss: 0.3029, validation loss: 0.2819\n",
      "Epoch [800], Loss: 0.2908, validation loss: 0.2697\n",
      "Epoch [900], Loss: 0.2808, validation loss: 0.2597\n",
      "Epoch [1000], Loss: 0.2724, validation loss: 0.2513\n",
      "Epoch [1100], Loss: 0.2652, validation loss: 0.2440\n",
      "Epoch [1200], Loss: 0.2588, validation loss: 0.2373\n",
      "Epoch [1300], Loss: 0.2533, validation loss: 0.2312\n",
      "Epoch [1400], Loss: 0.2484, validation loss: 0.2257\n",
      "Epoch [1500], Loss: 0.2440, validation loss: 0.2208\n",
      "Epoch [1600], Loss: 0.2400, validation loss: 0.2163\n",
      "Epoch [1700], Loss: 0.2361, validation loss: 0.2121\n",
      "Epoch [1800], Loss: 0.2323, validation loss: 0.2082\n",
      "Epoch [1900], Loss: 0.2286, validation loss: 0.2045\n",
      "Epoch [2000], Loss: 0.2250, validation loss: 0.2011\n",
      "Epoch [2100], Loss: 0.2217, validation loss: 0.1979\n",
      "Epoch [2200], Loss: 0.2186, validation loss: 0.1949\n",
      "Epoch [2300], Loss: 0.2156, validation loss: 0.1922\n",
      "Epoch [2400], Loss: 0.2129, validation loss: 0.1898\n",
      "Epoch [2500], Loss: 0.2104, validation loss: 0.1876\n",
      "Epoch [2600], Loss: 0.2080, validation loss: 0.1858\n",
      "Epoch [2700], Loss: 0.2058, validation loss: 0.1842\n",
      "Epoch [2800], Loss: 0.2038, validation loss: 0.1826\n",
      "Epoch [2900], Loss: 0.2019, validation loss: 0.1812\n",
      "Epoch [3000], Loss: 0.2001, validation loss: 0.1798\n",
      "Epoch [3100], Loss: 0.1984, validation loss: 0.1784\n",
      "Epoch [3200], Loss: 0.1967, validation loss: 0.1770\n",
      "Epoch [3300], Loss: 0.1951, validation loss: 0.1757\n",
      "Epoch [3400], Loss: 0.1935, validation loss: 0.1743\n",
      "Epoch [3500], Loss: 0.1919, validation loss: 0.1729\n",
      "Epoch [3600], Loss: 0.1903, validation loss: 0.1713\n",
      "Epoch [3700], Loss: 0.1889, validation loss: 0.1702\n",
      "Epoch [3800], Loss: 0.1877, validation loss: 0.1689\n",
      "Epoch [3900], Loss: 0.1864, validation loss: 0.1678\n",
      "Epoch [4000], Loss: 0.1852, validation loss: 0.1668\n",
      "Epoch [4100], Loss: 0.1839, validation loss: 0.1656\n",
      "Epoch [4200], Loss: 0.1828, validation loss: 0.1642\n",
      "Epoch [4300], Loss: 0.1817, validation loss: 0.1629\n",
      "Epoch [4400], Loss: 0.1807, validation loss: 0.1615\n",
      "Epoch [4500], Loss: 0.1797, validation loss: 0.1604\n",
      "Epoch [4600], Loss: 0.1788, validation loss: 0.1596\n",
      "Epoch [4700], Loss: 0.1780, validation loss: 0.1589\n",
      "Epoch [4800], Loss: 0.1772, validation loss: 0.1583\n",
      "Epoch [4900], Loss: 0.1765, validation loss: 0.1578\n",
      "Epoch [5000], Loss: 0.1758, validation loss: 0.1571\n",
      "Epoch [5100], Loss: 0.1751, validation loss: 0.1566\n",
      "Epoch [5200], Loss: 0.1745, validation loss: 0.1560\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.2789, validation loss: 1.2753\n",
      "Epoch [200], Loss: 0.6551, validation loss: 0.6431\n",
      "Epoch [300], Loss: 0.4488, validation loss: 0.4323\n",
      "Epoch [400], Loss: 0.3715, validation loss: 0.3524\n",
      "Epoch [500], Loss: 0.3345, validation loss: 0.3141\n",
      "Epoch [600], Loss: 0.3124, validation loss: 0.2913\n",
      "Epoch [700], Loss: 0.2972, validation loss: 0.2756\n",
      "Epoch [800], Loss: 0.2855, validation loss: 0.2633\n",
      "Epoch [900], Loss: 0.2759, validation loss: 0.2533\n",
      "Epoch [1000], Loss: 0.2675, validation loss: 0.2445\n",
      "Epoch [1100], Loss: 0.2600, validation loss: 0.2364\n",
      "Epoch [1200], Loss: 0.2532, validation loss: 0.2290\n",
      "Epoch [1300], Loss: 0.2470, validation loss: 0.2220\n",
      "Epoch [1400], Loss: 0.2414, validation loss: 0.2159\n",
      "Epoch [1500], Loss: 0.2364, validation loss: 0.2105\n",
      "Epoch [1600], Loss: 0.2317, validation loss: 0.2057\n",
      "Epoch [1700], Loss: 0.2276, validation loss: 0.2015\n",
      "Epoch [1800], Loss: 0.2239, validation loss: 0.1978\n",
      "Epoch [1900], Loss: 0.2206, validation loss: 0.1945\n",
      "Epoch [2000], Loss: 0.2176, validation loss: 0.1917\n",
      "Epoch [2100], Loss: 0.2149, validation loss: 0.1891\n",
      "Epoch [2200], Loss: 0.2124, validation loss: 0.1869\n",
      "Epoch [2300], Loss: 0.2102, validation loss: 0.1848\n",
      "Epoch [2400], Loss: 0.2081, validation loss: 0.1830\n",
      "Epoch [2500], Loss: 0.2061, validation loss: 0.1812\n",
      "Epoch [2600], Loss: 0.2042, validation loss: 0.1794\n",
      "Epoch [2700], Loss: 0.2024, validation loss: 0.1776\n",
      "Epoch [2800], Loss: 0.2007, validation loss: 0.1758\n",
      "Epoch [2900], Loss: 0.1991, validation loss: 0.1741\n",
      "Epoch [3000], Loss: 0.1975, validation loss: 0.1726\n",
      "Epoch [3100], Loss: 0.1960, validation loss: 0.1711\n",
      "Epoch [3200], Loss: 0.1945, validation loss: 0.1697\n",
      "Epoch [3300], Loss: 0.1930, validation loss: 0.1683\n",
      "Epoch [3400], Loss: 0.1916, validation loss: 0.1669\n",
      "Epoch [3500], Loss: 0.1902, validation loss: 0.1655\n",
      "Epoch [3600], Loss: 0.1889, validation loss: 0.1641\n",
      "Epoch [3700], Loss: 0.1878, validation loss: 0.1630\n",
      "Epoch [3800], Loss: 0.1868, validation loss: 0.1620\n",
      "Epoch [3900], Loss: 0.1858, validation loss: 0.1612\n",
      "Epoch [4000], Loss: 0.1849, validation loss: 0.1602\n",
      "Epoch [4100], Loss: 0.1840, validation loss: 0.1594\n",
      "Epoch [4200], Loss: 0.1831, validation loss: 0.1588\n",
      "Epoch [4300], Loss: 0.1823, validation loss: 0.1581\n",
      "Epoch [4400], Loss: 0.1816, validation loss: 0.1575\n",
      "Epoch [4500], Loss: 0.1810, validation loss: 0.1570\n",
      "Epoch [4600], Loss: 0.1803, validation loss: 0.1566\n",
      "Epoch [4700], Loss: 0.1797, validation loss: 0.1563\n",
      "Epoch [4800], Loss: 0.1790, validation loss: 0.1560\n",
      "Epoch [4900], Loss: 0.1784, validation loss: 0.1555\n",
      "Epoch [5000], Loss: 0.1777, validation loss: 0.1548\n",
      "Epoch [5100], Loss: 0.1771, validation loss: 0.1542\n",
      "Epoch [5200], Loss: 0.1764, validation loss: 0.1537\n",
      "Epoch [5300], Loss: 0.1758, validation loss: 0.1534\n",
      "Epoch [5400], Loss: 0.1752, validation loss: 0.1531\n",
      "Epoch [5500], Loss: 0.1745, validation loss: 0.1528\n",
      "Epoch [5600], Loss: 0.1740, validation loss: 0.1523\n",
      "Epoch [5700], Loss: 0.1735, validation loss: 0.1515\n",
      "Epoch [5800], Loss: 0.1730, validation loss: 0.1511\n",
      "Epoch [5900], Loss: 0.1725, validation loss: 0.1507\n",
      "Epoch [6000], Loss: 0.1721, validation loss: 0.1501\n",
      "Epoch [6100], Loss: 0.1716, validation loss: 0.1497\n",
      "Early stopping\n",
      "Epoch [100], Loss: 1.2586, validation loss: 1.2505\n",
      "Epoch [200], Loss: 0.6384, validation loss: 0.6260\n",
      "Epoch [300], Loss: 0.4497, validation loss: 0.4324\n",
      "Epoch [400], Loss: 0.3746, validation loss: 0.3543\n",
      "Epoch [500], Loss: 0.3356, validation loss: 0.3139\n",
      "Epoch [600], Loss: 0.3116, validation loss: 0.2893\n",
      "Epoch [700], Loss: 0.2947, validation loss: 0.2722\n",
      "Epoch [800], Loss: 0.2818, validation loss: 0.2592\n",
      "Epoch [900], Loss: 0.2713, validation loss: 0.2485\n",
      "Epoch [1000], Loss: 0.2625, validation loss: 0.2391\n",
      "Epoch [1100], Loss: 0.2550, validation loss: 0.2311\n",
      "Epoch [1200], Loss: 0.2484, validation loss: 0.2241\n",
      "Epoch [1300], Loss: 0.2427, validation loss: 0.2178\n",
      "Epoch [1400], Loss: 0.2377, validation loss: 0.2123\n",
      "Epoch [1500], Loss: 0.2331, validation loss: 0.2072\n",
      "Epoch [1600], Loss: 0.2289, validation loss: 0.2027\n",
      "Epoch [1700], Loss: 0.2252, validation loss: 0.1989\n",
      "Epoch [1800], Loss: 0.2217, validation loss: 0.1953\n",
      "Epoch [1900], Loss: 0.2184, validation loss: 0.1919\n",
      "Epoch [2000], Loss: 0.2153, validation loss: 0.1889\n",
      "Epoch [2100], Loss: 0.2125, validation loss: 0.1860\n",
      "Epoch [2200], Loss: 0.2099, validation loss: 0.1835\n",
      "Epoch [2300], Loss: 0.2075, validation loss: 0.1811\n",
      "Epoch [2400], Loss: 0.2053, validation loss: 0.1790\n",
      "Epoch [2500], Loss: 0.2032, validation loss: 0.1770\n",
      "Epoch [2600], Loss: 0.2013, validation loss: 0.1752\n",
      "Epoch [2700], Loss: 0.1996, validation loss: 0.1735\n",
      "Epoch [2800], Loss: 0.1980, validation loss: 0.1720\n",
      "Epoch [2900], Loss: 0.1965, validation loss: 0.1706\n",
      "Epoch [3000], Loss: 0.1949, validation loss: 0.1689\n",
      "Epoch [3100], Loss: 0.1935, validation loss: 0.1675\n",
      "Epoch [3200], Loss: 0.1920, validation loss: 0.1659\n",
      "Epoch [3300], Loss: 0.1907, validation loss: 0.1646\n",
      "Epoch [3400], Loss: 0.1894, validation loss: 0.1636\n",
      "Epoch [3500], Loss: 0.1882, validation loss: 0.1623\n",
      "Epoch [3600], Loss: 0.1870, validation loss: 0.1611\n",
      "Epoch [3700], Loss: 0.1859, validation loss: 0.1599\n",
      "Epoch [3800], Loss: 0.1849, validation loss: 0.1590\n",
      "Epoch [3900], Loss: 0.1839, validation loss: 0.1582\n",
      "Epoch [4000], Loss: 0.1830, validation loss: 0.1575\n",
      "Epoch [4100], Loss: 0.1821, validation loss: 0.1570\n",
      "Epoch [4200], Loss: 0.1813, validation loss: 0.1565\n",
      "Epoch [4300], Loss: 0.1805, validation loss: 0.1558\n",
      "Epoch [4400], Loss: 0.1797, validation loss: 0.1553\n",
      "Epoch [4500], Loss: 0.1789, validation loss: 0.1547\n",
      "Epoch [4600], Loss: 0.1781, validation loss: 0.1542\n",
      "Epoch [4700], Loss: 0.1772, validation loss: 0.1534\n",
      "Epoch [4800], Loss: 0.1764, validation loss: 0.1525\n",
      "Epoch [4900], Loss: 0.1755, validation loss: 0.1516\n",
      "Epoch [5000], Loss: 0.1747, validation loss: 0.1507\n",
      "Epoch [5100], Loss: 0.1739, validation loss: 0.1496\n",
      "Epoch [5200], Loss: 0.1731, validation loss: 0.1487\n",
      "Epoch [5300], Loss: 0.1723, validation loss: 0.1480\n",
      "Epoch [5400], Loss: 0.1715, validation loss: 0.1475\n",
      "Epoch [5500], Loss: 0.1707, validation loss: 0.1468\n",
      "Epoch [5600], Loss: 0.1699, validation loss: 0.1461\n",
      "Epoch [5700], Loss: 0.1693, validation loss: 0.1455\n",
      "Epoch [5800], Loss: 0.1687, validation loss: 0.1450\n",
      "Epoch [5900], Loss: 0.1681, validation loss: 0.1443\n",
      "Epoch [6000], Loss: 0.1675, validation loss: 0.1437\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "for i in range(20):\n",
    "    # Hyperparameters\n",
    "    input_dim = train_features_normalized.shape[1]\n",
    "    output_dim = len(set(train_labels))  # Assuming train_labels are class indices\n",
    "    hidden_dim = (input_dim + output_dim) // 2 + i\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = SimpleNN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    model, epochs, val_loss = train(model)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert the models to a scikit-learn MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/kp/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def pytorch_to_sklearn(pytorch_model):\n",
    "\n",
    "    # Extract weights and biases from PyTorch model\n",
    "    fc1_weight = pytorch_model.fc1.weight.data\n",
    "    fc1_bias = pytorch_model.fc1.bias.data\n",
    "    fc2_weight = pytorch_model.fc2.weight.data\n",
    "    fc2_bias = pytorch_model.fc2.bias.data\n",
    "\n",
    "    # Get the sizes for initialization\n",
    "    input_size = fc1_weight.shape[1]\n",
    "    hidden_size = fc1_weight.shape[0]\n",
    "    output_size = fc2_weight.shape[0]\n",
    "\n",
    "    # Initialize sklearn MLP\n",
    "    sklearn_mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(hidden_size,), activation=\"relu\", max_iter=1\n",
    "    )\n",
    "\n",
    "    # To ensure the model doesn't change the weights during the dummy fit, we set warm_start=True\n",
    "    sklearn_mlp.warm_start = True\n",
    "\n",
    "    # Dummy fit to initialize weights (necessary step before setting the weights)\n",
    "    sklearn_mlp.fit(np.zeros((output_size, input_size)), list(range(output_size)))\n",
    "\n",
    "    # Set the weights and biases\n",
    "    sklearn_mlp.coefs_[0] = fc1_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[0] = fc1_bias.numpy()\n",
    "    sklearn_mlp.coefs_[1] = fc2_weight.t().numpy()\n",
    "    sklearn_mlp.intercepts_[1] = fc2_bias.numpy()\n",
    "\n",
    "    return sklearn_mlp\n",
    "\n",
    "converted_models = []\n",
    "\n",
    "for model in models:\n",
    "    converted_models.append(pytorch_to_sklearn(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's transpile the models to Leo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n"
     ]
    }
   ],
   "source": [
    "from zkml import LeoTranspiler\n",
    "\n",
    "for i, converted_model in enumerate(converted_models):\n",
    "    # Transpile the deceision tree into Leo code\n",
    "    print(type(converted_model))\n",
    "    lt = LeoTranspiler(\n",
    "        model=converted_model, validation_data=train_features_normalized[0:600].numpy()\n",
    "    )\n",
    "    leo_project_path = os.path.join(os.getcwd(), \"modelresults\")\n",
    "    leo_project_name = f\"model_{i}_added_hidden_neurons\"\n",
    "    lt.to_leo(\n",
    "        path=leo_project_path, project_name=leo_project_name, fixed_point_scaling_factor=16\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
